<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tommy Jones</title>
    <link>https://www.jonesingfordata.com/</link>
      <atom:link href="https://www.jonesingfordata.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Tommy Jones</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Thomas W. Jones 2020</copyright><lastBuildDate>Sat, 08 Feb 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://www.jonesingfordata.com/img/icon-192.png</url>
      <title>Tommy Jones</title>
      <link>https://www.jonesingfordata.com/</link>
    </image>
    
    <item>
      <title>Blogging my way through the rest of my PhD</title>
      <link>https://www.jonesingfordata.com/post/2020-02-08-blogging-through-my-phd/</link>
      <pubDate>Sat, 08 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/post/2020-02-08-blogging-through-my-phd/</guid>
      <description>


&lt;p&gt;I’ve reached a turning point in my PhD studies.
Classes are behind me and what lies ahead is largely unstructured. Success or failure will largely be a function of whether or not (and how well) I pull myself together.&lt;/p&gt;
&lt;p&gt;The next steps are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Pass comprehensive exams AKA “comps” (scheduled for early April 2020)&lt;/li&gt;
&lt;li&gt;Propose and defend a dissertation topic (which must be completed by spring of 2021)&lt;/li&gt;
&lt;li&gt;Write and defend the dissertation itself&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Everyone knows that dissertation research is unstructured.
The lack of structure is—from what I can tell—a large source of struggle for PhD students.
(Another, unfortunately, is lack of support from their advisors and program.
Thankfully, I’ve been blessed with an amazing mentor and advisor in &lt;a href=&#34;https://cos.gmu.edu/cds/faculty-profile-william-kennedy/.&#34;&gt;Bill Kennedy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yet in my program—&lt;a href=&#34;https://cos.gmu.edu/cds/phd-in-computational-sciences-and-informatics/&#34;&gt;Computational Sciences and Informatics (CSI) at George Mason University&lt;/a&gt;—even comps are unstructured.
The program of study is largely a “choose your own adventure” degree.
As a result, there is no set exam that every student takes. Instead you form a committee as you finish classes and then more-or-less negotiate the subjects of your exam with your committee members.
There is some structure, but the content itself is bespoke based on courses you’ve taken and your intended dissertation focus.&lt;/p&gt;
&lt;p&gt;I have been taught (and reinforced by experience) that you don’t just write to explain; you write to understand.
To that end, I intend to start blogging again, but this time with focus.
My goal is to blog about what I’m studying for comps, what I’m reading related to my research, and what I’m thinking about.
(That last bit is to help articulate and codify some of the ideas knocking around in my head.)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://media.giphy.com/media/WOYKaXG2xJsBO/giphy.gif&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://media.giphy.com/media/WOYKaXG2xJsBO/giphy.gif&#34; width=&#34;50%&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Comprehensive exam study guide</title>
      <link>https://www.jonesingfordata.com/post/2020-02-08-comps-study-guide/</link>
      <pubDate>Sat, 08 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/post/2020-02-08-comps-study-guide/</guid>
      <description>


&lt;p&gt;This post is going to make a boring read.&lt;/p&gt;
&lt;p&gt;As I explained &lt;a href=&#34;https://www.jonesingfordata.com/post/2020-02-08-blogging-through-my-phd/&#34;&gt;in my last post&lt;/a&gt;, my program’s comprehensive exams are bespoke for the student.
I’m going to use this post as a high-level study guide to keep me on track.
I’ll update it as I get more clarification etc.&lt;/p&gt;
&lt;p&gt;The basic structure of the exam is as follows:&lt;/p&gt;
&lt;p&gt;There is a one-day (up to six-hour) written exam taken in-person at the university and proctored by one committee member.
The results are classified as “Pass”, “Fail”, or “Oral Exam Needed”.
The latter is used if I need to clarify or expand on my written answer.
The exam questions are submitted by 3 of my 5 committee members.
The questions are designed to track courses that I took as part of my degree “concentration” courses.&lt;/p&gt;
&lt;p&gt;After the written portion, there is a computational exam which I will have two weeks to complete.
Two of my committee members are taking on putting together this portion of the exam.&lt;/p&gt;
&lt;p&gt;Finally, I will convene in-person with my committee and (a) present on the results of the computational portion of the exam and (b) undergo any oral examination needed from the answers to my written questions.&lt;/p&gt;
&lt;p&gt;As a personal aside/musing: looking at what I’m being tested on, it feels like my degree concentration is more “machine learning” than “computational statistics”. Though, TBH these days, I feel like that’s more of a cultural statement than a mathematical one. I still self identify as a statistician and regularly attend JSM. ¯\&lt;em&gt;(ツ)&lt;/em&gt;/¯&lt;/p&gt;
&lt;div id=&#34;the-written-portion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The written portion&lt;/h2&gt;
&lt;p&gt;The courses we chose to base the written portion of the exam were&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://seor.vse.gmu.edu/~klaskey/SYST664/SYST664.html&#34;&gt;Bayesian Inference and Decision Theory&lt;/a&gt; which I actually took at &lt;a href=&#34;https://myaccess.georgetown.edu/pls/bninbp/bwckctlg.p_disp_course_detail?cat_term_in=201810&amp;amp;subj_code_in=MATH&amp;amp;crse_numb_in=640&#34;&gt;Georgetown&lt;/a&gt; through &lt;a href=&#34;https://www.consortium.org/&#34;&gt;the Consortium of Universities in the Washington Metropolitan Area&lt;/a&gt;, which lets me take comparable courses at any university with less buraucracy than using transfer credits.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://patriotweb.gmu.edu:9977/pls/prod/bwckctlg.p_disp_course_detail?cat_term_in=201670&amp;amp;subj_code_in=CSI&amp;amp;crse_numb_in=777&#34;&gt;Principles of Knowledge Mining&lt;/a&gt; which is really a machine learning course but focused on data mining&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://math.gmu.edu/syllabi/F17/689-001-Griva.pdf&#34;&gt;Computational Learning and Discovery&lt;/a&gt; which is also a machine learning course but focused more on the mathematics of the various methods&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since Principles of Knowledge Mining and Computational Learning and Discovery had significant overlap in material we decided to split the exam based on supervised learning and unsupervised learning, without distinction of which course it came from.&lt;/p&gt;
&lt;div id=&#34;baysian-stats&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Baysian stats&lt;/h3&gt;
&lt;p&gt;Technically, my main study resource for this will be the notes, homework, and exams from my Bayesian stats class. (This is an advantage of having the instructor for the course on your committee.) However, two books that I may use as additional reference are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/&#34;&gt;Bayesian Data Analysis&lt;/a&gt; by Gelman et al.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/book/10.1007/978-0-387-92407-6&#34;&gt;A First Course in Bayesian Statistical Methods&lt;/a&gt; by Hoff&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And I’ve been told that everything in the course is fair game. That said, I’m going to focus on the areas that feel the most rusty to me, namely&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Metropolis Hastings (this still seems like magic to me)&lt;/li&gt;
&lt;li&gt;Model checking and evaluation&lt;/li&gt;
&lt;li&gt;Baysian regression - linear and ridge regression&lt;/li&gt;
&lt;li&gt;The Dirichlet multinomial&lt;/li&gt;
&lt;li&gt;The Dirichlet multinomial with a hierarchical uniform prior&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;WRT that last two, I definitely know they’re going to come into play during my dissertation itself.
I’m planning to (a) implement a NUTS sampler for LDA as a derivative of MH sampler in the WarpLDA algorithm and (b) would like to implement an LDA derivative that puts a hierarchical uniform prior on &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\alpha\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;supervised-learning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Supervised learning&lt;/h3&gt;
&lt;p&gt;This section is still a bit “TBD” but what I know for sure follows:&lt;/p&gt;
&lt;p&gt;The books I’m using here are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/~hastie/Papers/ESLII.pdf&#34;&gt;The Elements of Statistical Learning&lt;/a&gt; by Hastie et al.&lt;/li&gt;
&lt;li&gt;TBD likely either Tan et al. (see below) or &lt;a href=&#34;https://www.cs.waikato.ac.nz/ml/weka/book.html&#34;&gt;Data Mining&lt;/a&gt; by Witten et al.&lt;/li&gt;
&lt;li&gt;I likely won’t be using &lt;a href=&#34;https://www.deeplearningbook.org/&#34;&gt;Deep Learning&lt;/a&gt; by Goodfellow et al. but in case someone comes across this page as a resource to study ML themselves, I feel I’d be remiss not to mention it. This is an excellent book for those of us with a more mathematical bent.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The topics I’ll be focusing on are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;SVMs from Hastie et. al (Ch. 12)&lt;/li&gt;
&lt;li&gt;TBD&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;unsupervised-learning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Unsupervised learning&lt;/h3&gt;
&lt;p&gt;The books I’m using here are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/~hastie/Papers/ESLII.pdf&#34;&gt;The Elements of Statistical Learning&lt;/a&gt; by Hastie et al.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www-users.cs.umn.edu/~kumar001/dmbook/index.php&#34;&gt;Introduction to Data Mining&lt;/a&gt; by Tan et al.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The topics I’ll be focusing on are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Clustering from Hastie et al. (Ch. 14, specifically section 14.3)&lt;/li&gt;
&lt;li&gt;Clustering from Tan et al. (Ch. 8 - 9 in 1st ed. TBD on whether or not I do 1st or 2nd ed.)&lt;/li&gt;
&lt;li&gt;Anomaly detection from Tan et al. (Ch. 10 in 1st ed.)&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;other-topics-in-machine-learning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other topics in machine learning&lt;/h3&gt;
&lt;p&gt;Ohter topics will be&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Overfitting&lt;/li&gt;
&lt;li&gt;Generalization&lt;/li&gt;
&lt;li&gt;Bias/variance tradeoff&lt;/li&gt;
&lt;li&gt;Model selection&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These are covered Hastie et al. Ch. 7.&lt;/p&gt;
&lt;p&gt;I am encouraged to look into the introductory chapters of &lt;a href=&#34;http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf&#34;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-computational-portion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The computational portion&lt;/h2&gt;
&lt;p&gt;I don’t really need to “study” for the computation portion.
From discussions, what we are going to do is do some flag planting for a paper that I intend to write as part of the dissertation.
I’ve implemented a couple forms of transfer learning for LDA (or an LDA-like model) in a &lt;a href=&#34;https://github.com/tommyjones/tidylda&#34;&gt;new R package I’m working on&lt;/a&gt;.
So we’d be looking at a preliminary study of that.
Thing is, we don’t know how permanant/forgetful LDA is in this paradigm.
And I don’t know that the way I implemented it is optimal.
Topics to consider might be&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Weighting/reweighting of the previous model’s topics in the prior of a new model.
This tunes how much the new model “remembers” the old model.&lt;/li&gt;
&lt;li&gt;Initialization strategies.
Algorithms for LDA shuffle around counts of document-token-topic assignments. If you want to “transfer” you should initialize your counts in proportion to that of the previously-trained model. Unfortunately, corpora don’t have the same number of tokens overall or per-document. So, how do you initialize those counts then? (FWIW, I am not convinced that the current way I did it in my in-development R package is the best strategy.)&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://www.jonesingfordata.com/about/</link>
      <pubDate>Sun, 10 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/about/</guid>
      <description>&lt;p&gt;My name is Tommy. I&amp;rsquo;m a member of the technical staff at In-Q-Tel and a coordinator for Data Science DC. I have an MS in mathematics and statistics from Georgetown University and a BA in economics from the College of William and Mary. I am also a PhD student in the George Mason University Department of Computational and Data Sciences. I am the author of the textmineR package for the R language. I am also a Marine Corps veteran. Terrible opinions are my own.&lt;/p&gt;

&lt;p&gt;Check out my &lt;a href=&#34;https://github.com/tommyjones&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can follow me on twitter &lt;a href=&#34;https://twitter.com/thos_jones&#34; target=&#34;_blank&#34;&gt;\@thos_jones&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optimizing Topic Models for Classification Tasks</title>
      <link>https://www.jonesingfordata.com/talk/2019_11_09_dcr/</link>
      <pubDate>Sat, 09 Nov 2019 10:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/talk/2019_11_09_dcr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Brief Introduction to Graph Theory</title>
      <link>https://www.jonesingfordata.com/talk/2019_04_15_dsdc/</link>
      <pubDate>Mon, 15 Apr 2019 18:30:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/talk/2019_04_15_dsdc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>https://www.jonesingfordata.com/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/publication/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://www.jonesingfordata.com/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/slides/example/</guid>
      <description>

&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;

&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Code block:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;

&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;

&lt;p&gt;Block math:&lt;/p&gt;

&lt;p&gt;$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;

&lt;p&gt;Make content appear incrementally&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
   One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three
&lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;

&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;


&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;


&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;

&lt;p&gt;Customize the slide style and background&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;

&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mining Texts with textmineR</title>
      <link>https://www.jonesingfordata.com/talk/2018_11_09_dcr/</link>
      <pubDate>Fri, 09 Nov 2018 10:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/talk/2018_11_09_dcr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>textmineR - NLP with R</title>
      <link>https://www.jonesingfordata.com/talk/2016_04_27_spdc/</link>
      <pubDate>Sun, 17 Apr 2016 18:30:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/talk/2016_04_27_spdc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>https://www.jonesingfordata.com/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/publication/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Topic Modeling with LDA and more</title>
      <link>https://www.jonesingfordata.com/talk/2014_11_12_dcnlp/</link>
      <pubDate>Wed, 12 Nov 2014 18:30:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/talk/2014_11_12_dcnlp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>https://www.jonesingfordata.com/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>marginal</title>
      <link>https://www.jonesingfordata.com/project/marginal/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/project/marginal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>mvrsquared</title>
      <link>https://www.jonesingfordata.com/project/mvrsquared/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/project/mvrsquared/</guid>
      <description></description>
    </item>
    
    <item>
      <title>R-squared for Topic Models</title>
      <link>https://www.jonesingfordata.com/project/rsquared/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/project/rsquared/</guid>
      <description></description>
    </item>
    
    <item>
      <title>textmineR</title>
      <link>https://www.jonesingfordata.com/project/textminer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/project/textminer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>tidylda</title>
      <link>https://www.jonesingfordata.com/project/tidylda/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/project/tidylda/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
