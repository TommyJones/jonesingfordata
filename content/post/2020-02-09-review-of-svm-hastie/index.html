---
title: "Support Vector Machines in Hastie et al."
author: "Tommy Jones"
date: "2020-02-09"
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: "2020-02-11"
featured: no
disable_jquery: no
projects: []
image:
  caption: 'Image from [Hastie et al. p. 418](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)'
  focal_point: ''
  preview_only: no
---



<p>In my effort to <a href="https://www.jonesingfordata.com/post/2020-02-08-blogging-through-my-phd/">blog my way through the rest of my PhD</a> and <a href="https://www.jonesingfordata.com/post/2020-02-08-comps-study-guide/">study for comps</a>, I present to you support vector machines.</p>
<p>When we covered SVMs in my ML class a couple years ago, we focused on computational methods rather than the math.
The focus for comps is more-or-less the opposite so we’re going with chapter 12 of <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">The Elements of Statistical Learning</a>.</p>
<p>I’ve found that many academics in CS seem infatuated with SVMs and I’ve struggled to understand why.</p>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
Can anyone explain to me why a great many academic CS folks really really like SVMs?
</p>
— Tommy Jones (<span class="citation">@thos_jones</span>) <a href="https://twitter.com/thos_jones/status/1226581814959099905?ref_src=twsrc%5Etfw">February 9, 2020</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Thankfully, Twitter was there for me when I needed it most.
Apparently, the answer is “kernel methods plus the math is nice”.</p>
<p>A warning for anyone reading this blog post: it’s probably terrible. :)
The introduction is probably a good way to think of SVMs.
Beyond that, I admit I dive into a lot of detail to help <em>me</em> work through it.
But I’m not sure that it’s any more useful than just getting your own copy of The Elements of Statistical Learning.</p>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>SVMs are one method to make a linear decision boundary for classification.
I hear you say “not all decision boundaries are linear”.
Well, dear reader, SVMs have an answer for you; read on.</p>
<p>There are two components to SVMs.</p>
<ol style="list-style-type: decimal">
<li>The support vector classifier and</li>
<li>Kernel methods</li>
</ol>
<p>The support vector classifier is what creates a linear boundary between classes.
(Or it creates a best guess at a linear boundary in the case of overlapping classes.)
I describe it in detail below.</p>
<p>Kernel methods are the solution to the fact that not all data are linear.
Basically, a “kernel method” is a projection from one space to another.
The hope (plan? theory?) is that this new space will lead to a linear separation between classes.
In many (most?) cases, the new space will be of higher dimension.
This can give us two things:
First, as already stated, non-linear can become linear.
Second, when classes are overlapping, a higher dimension can give them more separability.
In the latter case, this could result in overfitting.
However, stick with me, dear reader, and we will see how SVMs address this issue.</p>
</div>
<div id="the-support-vector-classifier" class="section level2">
<h2>The Support Vector Classifier</h2>
<p>Remember this:
<em>The support vector classifier finds the linear hyperplane that separates classes with the maximum margin.</em>
The image at the top shows this margin in the case of separable classes (left) and overlapping classes (right).</p>
<p>Some definitions you’ll need to follow the math are:</p>
<p><span class="math display">\[\begin{align}
  \text{outcomes: } &amp; \{y: y_i \in \{-1,1\}\}\\
  \text{features: } &amp; \{x: x_i \in \mathbb{R}^p\}\\
  \text{hyperplane: } &amp; \{x: f(x) = x^T\beta + \beta_0\}\\
  \text{classification rule: } &amp; G(x) = \text{sign}(x^T\beta + \beta_0)\\
  \text{margin: } &amp; M = \frac{1}{\lVert\beta\rVert}
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(\lVert \beta \rVert = 1\)</span>, meaning that <span class="math inline">\(\beta\)</span> is a unit vector.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>Now we have two optimization problems to consider:
In the trivial case, the <span class="math inline">\(x_i\)</span> are separable by class, <span class="math inline">\(y_i\)</span>.
So then it’s just an issue of finding the “right” hyperplane.
In the more realistic case, the <span class="math inline">\(x_i\)</span> are not completely separated by class.
This is a more difficult problem and requires a fancier solution.</p>
<div id="separable-classes" class="section level3">
<h3>Separable classes</h3>
<p>For the separable case, we have a basic optimization problem:</p>
<center>
<span class="math inline">\(\max_{\beta,\beta_0,\lVert\beta\rVert}M\)</span> subject to <span class="math inline">\(y_i \cdot (x_i^T\beta + \beta_0) \geq M\)</span>
</center>
<p>According to Hastie et al. this can be rephrased and more easily solved by</p>
<center>
<span class="math inline">\(\min_{\beta,\beta_0}\lVert\beta\rVert\)</span> subject to <span class="math inline">\(y_i\cdot(x^T\beta + \beta_0) \ge 1\)</span>
</center>
<div id="a-total-aside-that-links-this-back-to-calculus" class="section level4">
<h4>A total aside that links this back to calculus</h4>
<p>You might not be able to solve this analytically, but if I recall my calculus, you’d solve</p>
<center>
<span class="math inline">\(\min_{\beta,\beta_0} \lVert\beta\rVert + \lambda(y_i\cdot(x^T\beta + \beta_0) - 1 - s^2)\)</span>
</center>
<p>Where <span class="math inline">\(s^2\)</span> is a variable introduced to handle inequalities a-la <a href="http://users.wpi.edu/~pwdavis/Courses/MA1024B10/1024_Lagrange_multipliers.pdf">here</a>.
And <span class="math inline">\(\lambda\)</span> is a Lagrange multiplier.</p>
</div>
</div>
<div id="non-separable-classes" class="section level3">
<h3>Non-separable classes</h3>
<p>When the classes aren’t separable, we have to introduce a new variable, called a “slack variable” <span class="math inline">\(\xi\)</span>.
(If—like me—you have trouble pronouncing <span class="math inline">\(\xi\)</span>, <a href="https://en.wikipedia.org/wiki/Xi_(letter)">it’s pronounced like “sigh”</a>.)
<span class="math inline">\(\xi\)</span> is a vector the same length as <span class="math inline">\(y\)</span> (and as many rows as <span class="math inline">\(x\)</span>).
Using this variable allows some points to be on the wrong side of the margin.
(See the right image, above.)</p>
<p>The standard way to modify the constraint in the face of a slack variable is this:</p>
<center>
<span class="math inline">\(y_i \cdot (x^T\beta + \beta_0) \ge M\cdot(1 - \xi_i)\)</span>
</center>
<p>But we have to more constraints on the total number of misclassified observations.
The new constraints are</p>
<p><span class="math display">\[\begin{align}
  \xi_i \ge 0 \forall i\\
  \sum_{i=1}^N \xi_i \leq K
\end{align}\]</span></p>
<p><strong>This leads to the way the support vector classifier is usually defined.</strong></p>
<p><span class="math display">\[\begin{align}
  \min\lVert\beta\rVert &amp;\text{  subject to  }
    \begin{cases}
      y_i(x_i^T\beta + \beta_0) \ge 1 - \xi_i \forall i,\\
      \xi_ \geq 0, \sum \xi_i \leq K
    \end{cases}
\end{align}\]</span></p>
<div id="another-calculus-linking-aside" class="section level4">
<h4>Another calculus linking aside</h4>
<p>From the link above, if I wanted to do this with calculus I would have the following:</p>
<center>
<span class="math inline">\(\min_{\beta,\beta_0} \lVert\beta\rVert + \lambda_1(y_i\cdot(x^T\beta + \beta_0) - 1 - s_1^2) + \lambda_2(\xi - s_2^2) + \lambda_3(\sum_i\xi - K + s_3^2)\)</span>
</center>
<p>Full disclosure: I’m not 100% sure about the plus sign on <span class="math inline">\(s_3^2\)</span>. Caveat emptor!</p>
</div>
</div>
<div id="solving-it-the-way-hastie-et-al.do" class="section level3">
<h3>Solving it the way Hastie et al. do</h3>
<p>Reader, I warn you that this section gets ugly and confusing.
Feel free to skip it unless you’re going to build your own support vector classifier from scratch.</p>
<p>Hastie et al. (and I assume the rest of the ML world) rely on a couple of tricks to make the support vector classifier more computationally tractable.</p>
<ol style="list-style-type: decimal">
<li>They restate the problem to something that makes the algebra a little nicer.</li>
<li>They restate the problem again in a way that makes it nicer to put in a quadratic optimization solver.</li>
</ol>
<p>To the second, some vocabulary:
The “primal” problem is the equation as originally stated.
The “dual” problem is an equivalent problem that, if solved, will result in the same answer.</p>
<p>To make it more computationally friendly, the problem above is often re-stated as</p>
<center>
<span class="math inline">\(\min_{\beta,\beta_0} \frac{1}{2}\lVert\beta\rVert^2 + C\sum_{i=1}^N\xi_i\)</span> subject to <span class="math inline">\(\xi_i \geq 0, y_i(x_i^T\beta + \beta_0) \geq 1 - \xi_i, \forall i\)</span>
</center>
<p>The Lagrange (primal) function becomes</p>
<center>
<span class="math inline">\(L_P = \frac{1}{2}\lVert\beta\rVert^2 + C\sum_{i=1}^N\xi_i - \sum_{i=1}^N \alpha_i(y_i(x_i^T\beta+\beta_0) - (1 - \xi_i)) - \sum_{i=1}^N\mu_i\xi_i\)</span>
</center>
<p>I’m inferring that <span class="math inline">\(\alpha_i\)</span> and <span class="math inline">\(\mu_i\)</span> are the Lagrange multipliers.
But maybe I’m wrong.
Caveat emptor again!</p>
<p>Setting the derivatives equal to zero and doing some <del>magic</del> math we see</p>
<p><span class="math display">\[\begin{align}
  \beta &amp;= \sum_{i=1}^N \alpha_i y_i x_i\\
  0 &amp;= \sum_{i=1}^N \alpha_i y_i  \\
  \alpha_i &amp;= C - \mu_i, \forall i
\end{align}\]</span></p>
<p>which you can substitute back in to get the dual problem:</p>
<center>
<span class="math inline">\(L_D = \sum_{i=1}^N\alpha_i - \frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_jy_iy_jx_i^Tx_j^T\)</span>
</center>
<p>The above is subject to several constraints.
The first couple are from our problem as we’ve already stated it:</p>
<p><span class="math display">\[\begin{align}
  &amp;0 \leq \alpha_i \leq C\\
  &amp;\text{ and }\\
  &amp;\sum_{i=1}^N\alpha_iy_i = 0
\end{align}\]</span></p>
<p>And the remainder are from the <a href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions">Karush–Kuhn–Tucker conditions</a>.
(I’m going to refer to them as the KKT conditions.)
The KKT conditions help with finding solutions to nonlinear optimization problems.
The additional constraints that they introduce are</p>
<p><span class="math display">\[\begin{align}
  \alpha_i(y_i(x_i^T\beta + \beta_0) - (1 - \xi_i)) &amp;= 0\\
  \mu_i\xi_i &amp;= 0\\
  y_i(x_i^T\beta+\beta_0) - (1 - \xi_i) &amp;\geq 0
\end{align}\]</span></p>
</div>
<div id="bringing-it-home" class="section level3">
<h3>Bringing it home</h3>
<p>Ignoring the computational details that we just painfully went through (and still weren’t enough to get you to actually compute it yourself):</p>
<p>The solution for <span class="math inline">\(\beta\)</span> has the form <span class="math inline">\(\hat\beta = \sum_{i=1}^N \hat\alpha_i y_i x_x\)</span>.
For the overwhelming majority of observations, <span class="math inline">\(i\)</span>, <span class="math inline">\(\hat\alpha_i = 0\)</span>.
The ones where <span class="math inline">\(\hat\alpha_i \neq 0\)</span> are due to the case where <span class="math inline">\((x_i^T\beta + \beta_0) - (1 - \xi_i)) = 0\)</span> exactly.
(Note the first and last of the KKT conditions.)</p>
<p>These points are called “support vectors” since <span class="math inline">\(\hat\beta\)</span> is represented by them alone.
Of those, some points lie exactly on the margin.
In that case <span class="math inline">\(\hat\xi_i=0\)</span> and consequently <span class="math inline">\(0 &lt; \hat\alpha_i &lt; C\)</span>.
We use these points to solve for <span class="math inline">\(\beta_0\)</span>, usually by averaging across them.
For the remainder <span class="math inline">\(\hat\xi_i&gt;0\)</span> and <span class="math inline">\(\hat\alpha_i = C\)</span>.</p>
<p>Finally, as indicated way up at the top, you need to use the sign of <span class="math inline">\(x_i^T\hat\beta + \hat\beta_0\)</span> to make a class assignment.</p>
</div>
</div>
<div id="next-time" class="section level2">
<h2>Next time</h2>
<p>Next time I’ll write about kernel methods for SVMs and how we use an extension of the support vector classifier to estimate SVMs.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I have two questions about this: 1. Why? 2. Does this include <span class="math inline">\(\beta_0\)</span>? It’s always seemed more elegant to me when writing linear equations to just do <span class="math inline">\(x^T\beta\)</span> where one vector of <span class="math inline">\(x\)</span> is ones. For linear regression, that has no consequences, but it might here (or for ridge or lasso regressions).<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
