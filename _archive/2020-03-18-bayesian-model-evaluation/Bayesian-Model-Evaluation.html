---
title: "Bayesian model evaluation and comparison"
author: "Tommy Jones"
date: "2020-03-14"
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: "2020-03-18"
featured: no
disable_jquery: no
projects: []
---



<p>Suppose you have a Bayesian statistical model.
How do you know it’s a good one?
How do you know it isn’t fundamentally misspecified?
How do you compare it to alternative model specifications?
This post covers the material I’m studying for my comprehensive exam around Bayesian model evaluation (and checking and comparison).</p>
<p>But beyond studying for comps, this material has inspired some thoughts related to topic modeling.
Most probabilistic topic models are Bayesian.
The core topic of my research is about building better topic models more consistently.
I wonder about the degree to which these methods would apply (meaningfully) to topic modeling.
Food for thought…</p>
<p>For the below, I use the following convetions:</p>
<ul>
<li><span class="math inline">\(X\)</span> is the data</li>
<li><span class="math inline">\(\theta\)</span> is the parameter (or parameters) being estimated</li>
<li><span class="math inline">\(\mathcal{L}()\)</span> is the likelihood function</li>
<li><span class="math inline">\(\mathcal{l}()\)</span> is the log of <span class="math inline">\(\mathcal{L}()\)</span></li>
<li><span class="math inline">\(B\)</span> is the number of samples drawn in a Markov chain</li>
</ul>
<div id="checking-convergence" class="section level2">
<h2>Checking convergence</h2>
<p>Most Bayesian models are fit using some sort of <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">MCMC</a> sampling.
I won’t get into the details in this post.
However, like backpropagation and other methods, MCMC is is iterative.
It iteratively samples from the posterior and then updates estimates based on what’s sampled.
This means that we need to ensure that the model has stabilized, or converged.
Convergence is when the <span class="math inline">\(j\)</span>-th sample is independent of the <span class="math inline">\(j-b\)</span>-th samples.
To do this, one generally discards the initial iterations (which we know will not be independent).
If your model has not converged, then there’s no point in doing any other evaluation, diagnostics, inference, etc. because you wouldn’t be able to differentiate between a poor evaluation due to model misspecification or because your model didn’t converge, for example.</p>
<p>Convergence is assessed on the last B iterations.
Typically convergence is assessed against the parameters being estimated.
However, for models like LDA (and other topic models), there are so many parameters, that checking them all is infeasible for both computational and storage reasons.
In this case, we typically assess convergence on an overall statistic, like the log likelihood.</p>
<p>The convergence methods I’ll cover here are:</p>
<ul>
<li>Geweke’s convergence statistic</li>
<li>Gelmen-Rubin convergence statistic</li>
<li>Autocorrelation function plots</li>
</ul>
<div id="geweke-convergence-statistic" class="section level3">
<h3>Geweke convergence statistic</h3>
<p>The Geweke convergence statistic is fairly straightforward.</p>
<ol style="list-style-type: decimal">
<li>Take two non-overlapping (and ordered) subsets of samples from your chain.
(e.g. Take the first 10% vs. the last 50% of the chain.)</li>
<li>Calculate the mean over each subset.</li>
<li>Perform a difference in means test.</li>
</ol>
<p>If the chain is stationary (i.e. if the model has converged) the means won’t be different.
One change here is that you have to use autocorrelation consistent standard error.
In the R function <code>coda::geweke.stat</code>, the <a href="http://ugrad.stat.ubc.ca/R/library/coda/html/geweke.diag.html">documentation</a> states that “[t]he standard error is estimated from the spectral density at zero and so takes into account any autocorrelation.”</p>
</div>
<div id="gelmen-rubin-statistic" class="section level3">
<h3>Gelmen-Rubin statistic</h3>
<p>This statistic is a bit more complicated.
The bottom line is that you calculate a statistic called a “scale reduction factor”, <span class="math inline">\(\hat{R}\)</span>, that must be less than 1.2 to show convergence.
Unlike the Geweke stat., you have to run more than one chain to calculate the Gelmen-Rubin statistic.</p>
<p>The procedure is as follows:</p>
<ol style="list-style-type: decimal">
<li>Run <span class="math inline">\(m \geq 2\)</span> chains of length <span class="math inline">\(b = 2n\)</span>.</li>
<li>Discard the first 50% (or first <span class="math inline">\(n\)</span>) samples from each chain.</li>
<li>Calculate the average within chain variance,
<span class="math inline">\(W = \frac{1}{m}\sum_{j = 1}^m s^2_j\)</span>
where <span class="math inline">\(s^2_j = \frac{1}{n - 1}\sum_{i=1}^n (\theta_{i,j} - \bar\theta_j)^2\)</span>.</li>
<li>Calculate the between chain variance,
<span class="math inline">\(B = \frac{n}{m - 1}\sum_{j=1}^m(\bar\theta_j - \bar{\bar\theta})\)</span>
where <span class="math inline">\(\bar{\bar\theta} = \frac{1}{m}\sum_{j = 1}^m \bar\theta_j\)</span>.</li>
<li>Calculate the variance of <span class="math inline">\(\hat\theta\)</span> as a combination of within chain and between chain variance,
<span class="math inline">\(\text{var}(\hat\theta) = \left(1 - \frac{1}{n}\right)W + \frac{1}{n}B\)</span>.</li>
<li>Calculate the scale reduction factor,
<span class="math inline">\(\hat{R} = \sqrt{\frac{\text{var}(\hat\theta)}{W}}\)</span>.</li>
<li>If <span class="math inline">\(\hat{R} \leq 1.2\)</span> then your chains have converged.</li>
</ol>
</div>
<div id="autocorrelation-function-plot" class="section level3">
<h3>Autocorrelation function plot</h3>
<p>This isn’t a convergence statistic per-se.
Rather, it displays how correlated (on average) a sample is to the <span class="math inline">\(k\)</span> samples preceeding it.
If a sample is corrlated with preceeding samples, then it’s unlikely that the samples are independent.</p>
<p>I won’t get into the math.
(TBH, I don’t expect to get tested on it.)
But below is some R code that demonstrates the ACF plot of an independent sample and a sample that is very much not independent.
In general, we want the correlations to be close to zero to show independence (and thus chain convergence).</p>
<p>First, generate a chain of independent samples, <code>x_ind</code> and a chain where the current sample depends on the previous sample, <code>x_dep</code>.</p>
<pre class="r"><code># generate an independent sample
x_ind &lt;- rnorm(n = 1000, mean = 0, sd = 1)

# generate a sample whose current value depends on preceeding values
x_dep &lt;- numeric(1000)

x_dep[1] &lt;- rnorm(1, 0, 1)

for (j in seq_along(x_dep)[-1]) {
  
  x_dep[j] &lt;- rnorm(1, mean = x_dep[j - 1], sd = 1)
  
}</code></pre>
<p>Now plot the independent ACF.</p>
<pre class="r"><code>acf(x_ind, main = &quot;ACF of chain of independent samples&quot;)</code></pre>
<p><img src="/post/2020-03-18-bayesian-model-evaluation/Bayesian-Model-Evaluation_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>And the non-independent ACF.</p>
<pre class="r"><code>acf(x_dep, main = &quot;ACF of chain of non-independent samples&quot;)</code></pre>
<p><img src="/post/2020-03-18-bayesian-model-evaluation/Bayesian-Model-Evaluation_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
</div>
<div id="checking-for-misspecification" class="section level2">
<h2>Checking for misspecification</h2>
<p>Misspecification evaluations check whether or not your model has poor properties.
If it does, maybe you need different prior parameters.
Maybe you need a different prior distribution.
Maybe you need a different likelihood distribution.
Of course, even if everything looks good, there’s no guarantee that your model still isn’t pathologically misspecified. Caveat emptor!</p>
<p>The methods discussed below are:</p>
<ul>
<li>Sensitivity analysis</li>
<li>Mixture model approach</li>
<li>External validation</li>
<li>Posterior predictive check</li>
</ul>
<div id="sensitivity-analysis" class="section level3">
<h3>Sensitivity analysis</h3>
<p>A sensitivity analysis examines how the posterior distribution changes based on changes to the model specification.
This may include changes in:</p>
<ul>
<li>The prior parameters</li>
<li>The prior distribution</li>
<li>The likelihood distribution</li>
</ul>
</div>
<div id="mixture-model-approach" class="section level3">
<h3>Mixture model approach</h3>
<p>When you have many different models (combinations of likelihoods and priors) that you think might be valid.
One strategy might be to combine them into an “exhaustive probability model”.
Here, your posterior, <span class="math inline">\(P(\theta|X, \text{etc.})\)</span>, is a linear combination of all the models you think might be valid.
Then you can perform a sensitivity analysis on the total posterior by tweaking elements of each individual input.</p>
<p>For each likelihood <span class="math inline">\(\mathcal{L}_j\)</span> and prior <span class="math inline">\(\pi_j\)</span> the posterior is</p>
<p><span class="math display">\[\begin{align}
  P(\theta|X, \text{other params.})
  &amp;\propto
    \lambda_1\mathcal{L}_1\pi_1 + 
    \lambda_2\mathcal{L}_2\pi_2 + 
    \lambda_3\mathcal{L}_3\pi_3 + ...
\end{align}\]</span></p>
<p>where <span class="math inline">\(\sum_j \lambda_j = 1\)</span>.</p>
</div>
<div id="external-validation" class="section level3">
<h3>External validation</h3>
<p>For those familiar with machine learning, this should be familiar.
How well does your Bayesian model predict held out data?
If it is a poor predictor, it is likely not well-specified.
Done.</p>
</div>
<div id="posterior-predictive-check" class="section level3">
<h3>Posterior predictive check</h3>
<p>Bayesian models are generative.
A posterior predictive check has you sampling from the posterior.
Then take your posterior samples and compare them to your data.
(My reference doesn’t specify training data or held out data.
Why not do both?)</p>
<div id="tail-area-probabilities" class="section level4">
<h4>Tail area probabilities</h4>
<p>You can use a test statistic to compare the posterior samples with the observed data.
For example, do they have different means and standard deviations?
Maybe you can do a chi-squared test for equality of distribution.</p>
<p>To make this Bayesian, draw many samples and calculate many test statistics.
You then get a distribution of statistics and can accept or reject a null hypothesis based on that.</p>
</div>
</div>
</div>
<div id="comparing-models" class="section level2">
<h2>Comparing models</h2>
<p>Sometimes you have two models.
Maybe they were fit on the same data.
Maybe they weren’t.
But the bottom line is, you want to know which one is better.
In frequentist statistics, one often uses the Akaike information criterion or AIC.
There are two information criterion that may be used for Bayesian models.
There is also the concept of a Bayes factor, which calculates support for one model over another.</p>
<p>Topics discussed here are:</p>
<ul>
<li>Deviance information criterion (DIC)</li>
<li>Wantanabe Akaike information criterion (WAIC)</li>
<li>Bayes factors</li>
</ul>
<div id="deviance-information-criterion" class="section level3">
<h3>Deviance information criterion</h3>
<p>Like AIC, DIC is negative and smaller values are better.
It’s kind of Bayesian in that it’s calculated across samples in the chain.
However it’s not fully Bayesian in that it conditions on a point estimate, <span class="math inline">\(\hat\theta\)</span>.
(<span class="math inline">\(\hat\theta\)</span> may be an average over part of your Markov chain or it might be the parameter estimate on the last iteration.)
You can calculate it by</p>
<p><span class="math display">\[\begin{align}
  DIC &amp;=
    -2 \log \left(\mathcal{L}(X|\hat\theta)\right) + 2 P_{DIC}\\
  \text{where}&amp;\\
  P_{DIC} &amp;=
    2\left(\mathcal{l}(X|\theta) - \frac{1}{B}\sum_{b = 1}^B\mathcal{l}(X|\theta^{(b)})\right)\\
    &amp;= 2 \text{var}(\mathcal{l}(X|\theta))
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathcal{l}()\)</span> is the log likelihood function and <span class="math inline">\(B\)</span> is the number of samples.</p>
</div>
<div id="wantanabe-akaike-information-criterion" class="section level3">
<h3>Wantanabe Akaike information criterion</h3>
<p>WAIC is like AIC and DIC in that smaller is better.
WAIC has the advantage of averaging over the posterior distribution rather than conditioning on a point estimate.
You can caluclate it by</p>
<p><span class="math display">\[\begin{align}
  WAIC
    &amp;=
    -2\text{lppd} + 2P_{WAIC}\\
  &amp;\text{where}\\
  \text{lppd}
    &amp;=
    \sum_{i = 1}^n \log\left(
      \frac{1}{B}\sum_{b=1}^B \mathcal{L}(X|\theta^{(b)}\right)\\
  &amp;\text{and}\\
  P_{WAIC}
    &amp;=
    \sum_{i=1}^n \frac{1}{B-1}\sum_{b=1}^B \left(
      \mathcal{l}(X_i|\theta^{(b)}) - \frac{1}{B}\sum_{b=1}^B \mathcal{l}(X_i|\theta^{(b)}\right)^2
\end{align}\]</span></p>
</div>
<div id="bayes-factors" class="section level3">
<h3>Bayes Factors</h3>
<p>Bayes factors give you evidence for one model over another, specifically one posterior over another.
i.e. for <span class="math inline">\(H_2\)</span> over <span class="math inline">\(H_1\)</span>, calculate<span class="math inline">\(\frac{P(H_2|X)}{P(H_1|X)} = \frac{H_2}{H_1}\times \text{BF}(H_2;H_1)\)</span>.
The goal when using Bayes Factors is to choose a single model <span class="math inline">\(H_i\)</span> or average over a discrete set using their posterior probabilities.
Bayes factors work well when the underlying model is truly discrete.</p>
<p>The Bayes factor of <span class="math inline">\(H_2\)</span> over <span class="math inline">\(H_1\)</span> is</p>
<p><span class="math display">\[\begin{align}
  \text{BF}(H_2;H_1)
    &amp;= \frac{P(X|H_2)}{P(X|H_1)}\\
    &amp;= \frac{\int P(\theta_2,X|H_2)d\theta_2}{\int P(\theta_1,X|H_1)d\theta_1}
\end{align}\]</span></p>
<p>You can interpret the Bayes factor using the following:</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(\textbf{BF}\boldsymbol{(H_2;H_1)}\)</span></th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(&lt; 1\)</span></td>
<td>Supports <span class="math inline">\(H_1\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(1 - 3.2\)</span></td>
<td>Approximate evidence for <span class="math inline">\(H_1\)</span> and <span class="math inline">\(H_2\)</span> are the same</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(3.2 - 10\)</span></td>
<td>Substantial support for <span class="math inline">\(H_2\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(10 - 31.6\)</span></td>
<td>Strong support for <span class="math inline">\(H_2\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(31.6 - 100\)</span></td>
<td>Very strong support for <span class="math inline">\(H_2\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(&gt; 100\)</span></td>
<td>Decisive support for <span class="math inline">\(H_2\)</span></td>
</tr>
</tbody>
</table>
<p>Bayes factors aren’t just a model comparison.
They’re a type of Bayesian inference since you are using the posteriors of two models to find evidence supporting one over the other.</p>
</div>
</div>
