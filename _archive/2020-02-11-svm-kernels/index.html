---
title: "Support vector machines and kernels"
author: "Tommy Jones"
date: "2020-02-10"
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: "2020-03-18"
featured: no
disable_jquery: no
projects: []
---



<p>In my effort to <a href="https://www.jonesingfordata.com/post/2020-02-08-blogging-through-my-phd/">blog my way through the rest of my PhD</a> and <a href="https://www.jonesingfordata.com/post/2020-02-08-comps-study-guide/">study for comps</a>, I present to you more on support vector machines.</p>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>As I said <a href="https://www.jonesingfordata.com/post/2020-02-09-review-of-svm-hastie/">last time</a>:
There are two components to SVMs.</p>
<ol style="list-style-type: decimal">
<li>The support vector classifier and</li>
<li>Kernel methods</li>
</ol>
<p>The first post covered the support vector classifier and its estimation.
In this post, I’m going to cover kernel methods for SVMs.</p>
</div>
<div id="how-do-we-get-non-linearity-from-a-linear-classifier" class="section level2">
<h2>How do we get non-linearity from a linear classifier?</h2>
<p>With SVMs, the intuition of addressing non-linearity is similar to linear regression.
Basically, you transform your right-hand-side variables in non-linear ways.
Remember that for linear regression, if my RHS variable is <span class="math inline">\(x\)</span> but I want it to have a quadratic relationship to <span class="math inline">\(y\)</span>, then I regress <span class="math inline">\(x^2\)</span> on <span class="math inline">\(y\)</span> instead of trying to figure out some sort of quadratic regression.
The intuition with SVMs are the same: transform <span class="math inline">\(x\)</span> in a non-linear way, then use the linear support vector classifier on it.</p>
<p>The intuition is the same, but of course it’s more complicated with SVMs.</p>
</div>
<div id="step-1-transform-your-space-transform-your-life" class="section level2">
<h2>Step 1: transform your space transform your life</h2>
<p>We want to project our features <span class="math inline">\(x\)</span> from their original space, <span class="math inline">\(\mathbb{R}^p\)</span>, to a new space, <span class="math inline">\(\mathbb{R}^q\)</span>.
And perhaps <span class="math inline">\(q &gt;&gt; p\)</span>.
We are going to do this with a function, <span class="math inline">\(h\)</span>, such that <span class="math inline">\(h: \mathbb{R}^p \mapsto \mathbb{R}^q\)</span></p>
<p>Then, we fit a linear classifier on <span class="math inline">\(h(x)\)</span> instead of <span class="math inline">\(x\)</span> like so:
<span class="math inline">\(f(x) = h(x)^T\beta + \beta_0\)</span>.</p>
<p>Similar to the dual problem that we did in my last post we need to maximize</p>
<center>
<span class="math inline">\(L_D = \sum_{i=1}^N\alpha_i - \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j y_iy_j \langle h(x_i), h(x_j) \rangle\)</span>
</center>
<p>where <span class="math inline">\(\langle a, b \rangle\)</span> represents the inner product between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
<p>Using the result from last time that <span class="math inline">\(\beta = \sum_{i=1}^N\alpha_i y_i x_i\)</span> and some substitution we see that</p>
<p><span class="math display">\[\begin{align}
  f(x) &amp;= h(x)^T\beta + \beta_0\\
       &amp;= \sum_{i=1}^N \alpha_i y_i \langle h(x), h(x_i)\rangle + \beta_0
\end{align}\]</span></p>
<p>and we can solve for <span class="math inline">\(\beta_0\)</span> by solving <span class="math inline">\(y_i f(x_i) = 1\)</span> for all of the <span class="math inline">\(x_i\)</span> where <span class="math inline">\(0 &lt; \alpha_i &lt; C\)</span>, or for the support vectors.</p>
</div>
<div id="step-2-what-is-a-kernel" class="section level2">
<h2>Step 2: what is a kernel?</h2>
<p>We can see that to solve this linear classifier with the projected data <span class="math inline">\(h(x)\)</span>, we need the inner products <span class="math inline">\(h(x)h(x)^T\)</span>.</p>
</div>
