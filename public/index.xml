<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tommy Jones</title>
    <link>https://www.jonesingfordata.com/</link>
      <atom:link href="https://www.jonesingfordata.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Tommy Jones</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Thomas W. Jones 2020</copyright><lastBuildDate>Sat, 14 Mar 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://www.jonesingfordata.com/img/icon-192.png</url>
      <title>Tommy Jones</title>
      <link>https://www.jonesingfordata.com/</link>
    </image>
    
    <item>
      <title>Bayesian model evaluation and comparison</title>
      <link>https://www.jonesingfordata.com/post/2020-03-18-bayesian-model-evaluation/bayesian-model-evaluation/</link>
      <pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/post/2020-03-18-bayesian-model-evaluation/bayesian-model-evaluation/</guid>
      <description>


&lt;p&gt;Suppose you have a Bayesian statistical model.
How do you know it’s a good one?
How do you know it isn’t fundamentally misspecified?
How do you compare it to alternative model specifications?
This post covers the material I’m studying for my comprehensive exam around Bayesian model evaluation (and checking and comparison).&lt;/p&gt;
&lt;p&gt;But beyond studying for comps, this material has inspired some thoughts related to topic modeling.
Most probabilistic topic models are Bayesian.
The core topic of my research is about building better topic models more consistently.
I wonder about the degree to which these methods would apply (meaningfully) to topic modeling.
Food for thought…&lt;/p&gt;
&lt;p&gt;For the below, I use the following convetions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the data&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is the parameter (or parameters) being estimated&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}()\)&lt;/span&gt; is the likelihood function&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{l}()\)&lt;/span&gt; is the log of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}()\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is the number of samples drawn in a Markov chain&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;checking-convergence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Checking convergence&lt;/h2&gt;
&lt;p&gt;Most Bayesian models are fit using some sort of &lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo&#34;&gt;MCMC&lt;/a&gt; sampling.
I won’t get into the details in this post.
However, like backpropagation and other methods, MCMC is is iterative.
It iteratively samples from the posterior and then updates estimates based on what’s sampled.
This means that we need to ensure that the model has stabilized, or converged.
Convergence is when the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th sample is independent of the &lt;span class=&#34;math inline&#34;&gt;\(j-b\)&lt;/span&gt;-th samples.
To do this, one generally discards the initial iterations (which we know will not be independent).
If your model has not converged, then there’s no point in doing any other evaluation, diagnostics, inference, etc. because you wouldn’t be able to differentiate between a poor evaluation due to model misspecification or because your model didn’t converge, for example.&lt;/p&gt;
&lt;p&gt;Convergence is assessed on the last B iterations.
Typically convergence is assessed against the parameters being estimated.
However, for models like LDA (and other topic models), there are so many parameters, that checking them all is infeasible for both computational and storage reasons.
In this case, we typically assess convergence on an overall statistic, like the log likelihood.&lt;/p&gt;
&lt;p&gt;The convergence methods I’ll cover here are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Geweke’s convergence statistic&lt;/li&gt;
&lt;li&gt;Gelmen-Rubin convergence statistic&lt;/li&gt;
&lt;li&gt;Autocorrelation function plots&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;geweke-convergence-statistic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Geweke convergence statistic&lt;/h3&gt;
&lt;p&gt;The Geweke convergence statistic is fairly straightforward.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Take two non-overlapping (and ordered) subsets of samples from your chain.
(e.g. Take the first 10% vs. the last 50% of the chain.)&lt;/li&gt;
&lt;li&gt;Calculate the mean over each subset.&lt;/li&gt;
&lt;li&gt;Perform a difference in means test.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the chain is stationary (i.e. if the model has converged) the means won’t be different.
One change here is that you have to use autocorrelation consistent standard error.
In the R function &lt;code&gt;coda::geweke.stat&lt;/code&gt;, the &lt;a href=&#34;http://ugrad.stat.ubc.ca/R/library/coda/html/geweke.diag.html&#34;&gt;documentation&lt;/a&gt; states that “[t]he standard error is estimated from the spectral density at zero and so takes into account any autocorrelation.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gelmen-rubin-statistic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gelmen-Rubin statistic&lt;/h3&gt;
&lt;p&gt;This statistic is a bit more complicated.
The bottom line is that you calculate a statistic called a “scale reduction factor”, &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt;, that must be less than 1.2 to show convergence.
Unlike the Geweke stat., you have to run more than one chain to calculate the Gelmen-Rubin statistic.&lt;/p&gt;
&lt;p&gt;The procedure is as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Run &lt;span class=&#34;math inline&#34;&gt;\(m \geq 2\)&lt;/span&gt; chains of length &lt;span class=&#34;math inline&#34;&gt;\(b = 2n\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Discard the first 50% (or first &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;) samples from each chain.&lt;/li&gt;
&lt;li&gt;Calculate the average within chain variance,
&lt;span class=&#34;math inline&#34;&gt;\(W = \frac{1}{m}\sum_{j = 1}^m s^2_j\)&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(s^2_j = \frac{1}{n - 1}\sum_{i=1}^n (\theta_{i,j} - \bar\theta_j)^2\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Calculate the between chain variance,
&lt;span class=&#34;math inline&#34;&gt;\(B = \frac{n}{m - 1}\sum_{j=1}^m(\bar\theta_j - \bar{\bar\theta})\)&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\bar{\bar\theta} = \frac{1}{m}\sum_{j = 1}^m \bar\theta_j\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Calculate the variance of &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta\)&lt;/span&gt; as a combination of within chain and between chain variance,
&lt;span class=&#34;math inline&#34;&gt;\(\text{var}(\hat\theta) = \left(1 - \frac{1}{n}\right)W + \frac{1}{n}B\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Calculate the scale reduction factor,
&lt;span class=&#34;math inline&#34;&gt;\(\hat{R} = \sqrt{\frac{\text{var}(\hat\theta)}{W}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\hat{R} \leq 1.2\)&lt;/span&gt; then your chains have converged.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;autocorrelation-function-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Autocorrelation function plot&lt;/h3&gt;
&lt;p&gt;This isn’t a convergence statistic per-se.
Rather, it displays how correlated (on average) a sample is to the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; samples preceeding it.
If a sample is corrlated with preceeding samples, then it’s unlikely that the samples are independent.&lt;/p&gt;
&lt;p&gt;I won’t get into the math.
(TBH, I don’t expect to get tested on it.)
But below is some R code that demonstrates the ACF plot of an independent sample and a sample that is very much not independent.
In general, we want the correlations to be close to zero to show independence (and thus chain convergence).&lt;/p&gt;
&lt;p&gt;First, generate a chain of independent samples, &lt;code&gt;x_ind&lt;/code&gt; and a chain where the current sample depends on the previous sample, &lt;code&gt;x_dep&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# generate an independent sample
x_ind &amp;lt;- rnorm(n = 1000, mean = 0, sd = 1)

# generate a sample whose current value depends on preceeding values
x_dep &amp;lt;- numeric(1000)

x_dep[1] &amp;lt;- rnorm(1, 0, 1)

for (j in seq_along(x_dep)[-1]) {
  
  x_dep[j] &amp;lt;- rnorm(1, mean = x_dep[j - 1], sd = 1)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now plot the independent ACF.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;acf(x_ind, main = &amp;quot;ACF of chain of independent samples&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jonesingfordata.com/post/2020-03-18-bayesian-model-evaluation/Bayesian-Model-Evaluation_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And the non-independent ACF.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;acf(x_dep, main = &amp;quot;ACF of chain of non-independent samples&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jonesingfordata.com/post/2020-03-18-bayesian-model-evaluation/Bayesian-Model-Evaluation_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-for-misspecification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Checking for misspecification&lt;/h2&gt;
&lt;p&gt;Misspecification evaluations check whether or not your model has poor properties.
If it does, maybe you need different prior parameters.
Maybe you need a different prior distribution.
Maybe you need a different likelihood distribution.
Of course, even if everything looks good, there’s no guarantee that your model still isn’t pathologically misspecified. Caveat emptor!&lt;/p&gt;
&lt;p&gt;The methods discussed below are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sensitivity analysis&lt;/li&gt;
&lt;li&gt;Mixture model approach&lt;/li&gt;
&lt;li&gt;External validation&lt;/li&gt;
&lt;li&gt;Posterior predictive check&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;sensitivity-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sensitivity analysis&lt;/h3&gt;
&lt;p&gt;A sensitivity analysis examines how the posterior distribution changes based on changes to the model specification.
This may include changes in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The prior parameters&lt;/li&gt;
&lt;li&gt;The prior distribution&lt;/li&gt;
&lt;li&gt;The likelihood distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;mixture-model-approach&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mixture model approach&lt;/h3&gt;
&lt;p&gt;When you have many different models (combinations of likelihoods and priors) that you think might be valid.
One strategy might be to combine them into an “exhaustive probability model”.
Here, your posterior, &lt;span class=&#34;math inline&#34;&gt;\(P(\theta|X, \text{etc.})\)&lt;/span&gt;, is a linear combination of all the models you think might be valid.
Then you can perform a sensitivity analysis on the total posterior by tweaking elements of each individual input.&lt;/p&gt;
&lt;p&gt;For each likelihood &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}_j\)&lt;/span&gt; and prior &lt;span class=&#34;math inline&#34;&gt;\(\pi_j\)&lt;/span&gt; the posterior is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  P(\theta|X, \text{other params.})
  &amp;amp;\propto
    \lambda_1\mathcal{L}_1\pi_1 + 
    \lambda_2\mathcal{L}_2\pi_2 + 
    \lambda_3\mathcal{L}_3\pi_3 + ...
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sum_j \lambda_j = 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;external-validation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;External validation&lt;/h3&gt;
&lt;p&gt;For those familiar with machine learning, this should be familiar.
How well does your Bayesian model predict held out data?
If it is a poor predictor, it is likely not well-specified.
Done.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-predictive-check&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Posterior predictive check&lt;/h3&gt;
&lt;p&gt;Bayesian models are generative.
A posterior predictive check has you sampling from the posterior.
Then take your posterior samples and compare them to your data.
(My reference doesn’t specify training data or held out data.
Why not do both?)&lt;/p&gt;
&lt;div id=&#34;tail-area-probabilities&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Tail area probabilities&lt;/h4&gt;
&lt;p&gt;You can use a test statistic to compare the posterior samples with the observed data.
For example, do they have different means and standard deviations?
Maybe you can do a chi-squared test for equality of distribution.&lt;/p&gt;
&lt;p&gt;To make this Bayesian, draw many samples and calculate many test statistics.
You then get a distribution of statistics and can accept or reject a null hypothesis based on that.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparing models&lt;/h2&gt;
&lt;p&gt;Sometimes you have two models.
Maybe they were fit on the same data.
Maybe they weren’t.
But the bottom line is, you want to know which one is better.
In frequentist statistics, one often uses the Akaike information criterion or AIC.
There are two information criterion that may be used for Bayesian models.
There is also the concept of a Bayes factor, which calculates support for one model over another.&lt;/p&gt;
&lt;p&gt;Topics discussed here are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deviance information criterion (DIC)&lt;/li&gt;
&lt;li&gt;Wantanabe Akaike information criterion (WAIC)&lt;/li&gt;
&lt;li&gt;Bayes factors&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;deviance-information-criterion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Deviance information criterion&lt;/h3&gt;
&lt;p&gt;Like AIC, DIC is negative and smaller values are better.
It’s kind of Bayesian in that it’s calculated across samples in the chain.
However it’s not fully Bayesian in that it conditions on a point estimate, &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta\)&lt;/span&gt;.
(&lt;span class=&#34;math inline&#34;&gt;\(\hat\theta\)&lt;/span&gt; may be an average over part of your Markov chain or it might be the parameter estimate on the last iteration.)
You can calculate it by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  DIC &amp;amp;=
    -2 \log \left(\mathcal{L}(X|\hat\theta)\right) + 2 P_{DIC}\\
  \text{where}&amp;amp;\\
  P_{DIC} &amp;amp;=
    2\left(\mathcal{l}(X|\theta) - \frac{1}{B}\sum_{b = 1}^B\mathcal{l}(X|\theta^{(b)})\right)\\
    &amp;amp;= 2 \text{var}(\mathcal{l}(X|\theta))
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{l}()\)&lt;/span&gt; is the log likelihood function and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is the number of samples.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wantanabe-akaike-information-criterion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wantanabe Akaike information criterion&lt;/h3&gt;
&lt;p&gt;WAIC is like AIC and DIC in that smaller is better.
WAIC has the advantage of averaging over the posterior distribution rather than conditioning on a point estimate.
You can caluclate it by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  WAIC
    &amp;amp;=
    -2\text{lppd} + 2P_{WAIC}\\
  &amp;amp;\text{where}\\
  \text{lppd}
    &amp;amp;=
    \sum_{i = 1}^n \log\left(
      \frac{1}{B}\sum_{b=1}^B \mathcal{L}(X|\theta^{(b)}\right)\\
  &amp;amp;\text{and}\\
  P_{WAIC}
    &amp;amp;=
    \sum_{i=1}^n \frac{1}{B-1}\sum_{b=1}^B \left(
      \mathcal{l}(X_i|\theta^{(b)}) - \frac{1}{B}\sum_{b=1}^B \mathcal{l}(X_i|\theta^{(b)}\right)^2
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayes-factors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bayes Factors&lt;/h3&gt;
&lt;p&gt;Bayes factors give you evidence for one model over another, specifically one posterior over another.
i.e. for &lt;span class=&#34;math inline&#34;&gt;\(H_2\)&lt;/span&gt; over &lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;, calculate&lt;span class=&#34;math inline&#34;&gt;\(\frac{P(H_2|X)}{P(H_1|X)} = \frac{H_2}{H_1}\times \text{BF}(H_2;H_1)\)&lt;/span&gt;.
The goal when using Bayes Factors is to choose a single model &lt;span class=&#34;math inline&#34;&gt;\(H_i\)&lt;/span&gt; or average over a discrete set using their posterior probabilities.
Bayes factors work well when the underlying model is truly discrete.&lt;/p&gt;
&lt;p&gt;The Bayes factor of &lt;span class=&#34;math inline&#34;&gt;\(H_2\)&lt;/span&gt; over &lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \text{BF}(H_2;H_1)
    &amp;amp;= \frac{P(X|H_2)}{P(X|H_1)}\\
    &amp;amp;= \frac{\int P(\theta_2,X|H_2)d\theta_2}{\int P(\theta_1,X|H_1)d\theta_1}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You can interpret the Bayes factor using the following:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{BF}\boldsymbol{(H_2;H_1)}\)&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;Interpretation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(&amp;lt; 1\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Supports &lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(1 - 3.2\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Approximate evidence for &lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H_2\)&lt;/span&gt; are the same&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(3.2 - 10\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Substantial support for &lt;span class=&#34;math inline&#34;&gt;\(H_2\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(10 - 31.6\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Strong support for &lt;span class=&#34;math inline&#34;&gt;\(H_2\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(31.6 - 100\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Very strong support for &lt;span class=&#34;math inline&#34;&gt;\(H_2\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(&amp;gt; 100\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Decisive support for &lt;span class=&#34;math inline&#34;&gt;\(H_2\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Bayes factors aren’t just a model comparison.
They’re a type of Bayesian inference since you are using the posteriors of two models to find evidence supporting one over the other.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Intro to clustering</title>
      <link>https://www.jonesingfordata.com/post/intro-to-clustering/</link>
      <pubDate>Tue, 11 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/post/intro-to-clustering/</guid>
      <description>


&lt;p&gt;Cluster analysis is a type (perhaps the most common type) of unsupervised machine learning.
In cluster analysis, the goal is to assign points to groups in such a way that points in the same group are more similar to each other than they are to points in other groups.&lt;/p&gt;
&lt;p&gt;One thing that is (somewhat shockingly) not explicitly stated in discussions of clustering is this:
Fundamental to cluster analysis is the notion of distance.
Specifically, we measure every point’s distance to something, whether it’s all the other points or to certain exemplar points.
In many cases, one can use the same clustering algorithm with different distance measures and get wildly different results.
I can tell you from my experience that a great many attempts at clustering have failed because of a poor choice of distance measure.
In my opinion, for example, cosine similarity (distance) is wildly over used and results may suffer.
But I think that’s probably a &lt;del&gt;rant&lt;/del&gt; post in its own right and better left off of my study guide.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Turns out Hastie et al. have a pretty big section leading into a discussion of clustering all on distance matrices and measures. Thus this shows again how great that book is.&lt;/p&gt;
&lt;p&gt;This post gives a high-level overview of clustering.
It is mostly drawn from the first edition of &lt;a href=&#34;https://www-users.cs.umn.edu/~kumar001/dmbook/index.php&#34;&gt;Introduction to Data Mining by Tan et al.&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Tan et al. put forth two different taxonomies of clustering algorithms.
I’m not going to lie: I struggle to define each of the taxonomies, though it’s obvious they are two different (and useful) ways of thinking about your clustering algorithms.&lt;/p&gt;
&lt;p&gt;This &lt;em&gt;particular&lt;/em&gt; post pulls from Tan et al. not &lt;a href=&#34;&#34;&gt;Hastie et al.&lt;/a&gt;
As a result, I can’t guarantee that all of Hastie’s stuff (which I will write about) fits neatly into these taxonomies.
However, I think it’s good to have a working (if imperfect) high-level framework for this type of thing.
Having that high-level framework makes it easier to incorporate new information, IMHO.&lt;/p&gt;
&lt;div id=&#34;taxonomy-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Taxonomy 1&lt;/h3&gt;
&lt;p&gt;This taxonomy breaks down as&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;em&gt;Hierarchical vs. partitional&lt;/em&gt;
Hierarchical clustering assumes that clusters are hierarchically nested. As one moves up the tree, smaller clusters merge into larger clusters. (Or you can move down and split larger clusters into smaller ones.) Partitional clustering cuts all the ponts into distinct groups.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Exclusive vs. overlapping vs. fuzzy&lt;/em&gt;
Exclusive clustering means that points are assigned to one and only one cluster. Overlapping clustering means that a point can simultaneously belong in more than one cluster at once. Fuzzy clustering means that each point is assigned a weighted (possibly probabilistic) membership to every cluster.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Complete vs. partial&lt;/em&gt;
A complete clustering assigns every point to a cluster. A partial clustering allows some points to belong in no clusters. An example of the latter is typical in density-based clustering where some points may be assigned as “noise” and not given a cluster assignment.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;taxonomy-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Taxonomy 2&lt;/h3&gt;
&lt;p&gt;This taxonomy breaks down as&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;em&gt;Well-separated&lt;/em&gt;
Well-separated clusters are the platonic ideal of a cluster. There are clear divisions between which points belong in which cluster(s).&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Prototype-based&lt;/em&gt;
In prototype-based clustering, each cluster has an exemplar or prototype. An example might be the mean or median of each point in a cluster. Then, points are assigned to clusters based on their distance to the prototype.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Graph-based&lt;/em&gt;
Graph-based clustering is often called “community detection.” If the data may be represented as a graph, with points being nodes, then the distance/similarity represents a link between points.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Density-based&lt;/em&gt;
In density-based clustering, a cluster is defined as a dense region of points surrounded by a low density region. Points in the low density region bay be consistered “noise points” and not assigned to any cluster.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Shared-property (conceptual clustering)&lt;/em&gt;
In this type of clustering scheme, points are assigned a cluster because they share a common property. For example, imagine assigning all red cars to a cluster, regardless of other similarities or differeneces they may have.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;further-on-clustering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Further on clustering&lt;/h3&gt;
&lt;p&gt;My following posts will outline several common clustering algorithms as well as cover cluster evaluation and some other topics in clustering. Stay tuned!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Support vector machines and kernels</title>
      <link>https://www.jonesingfordata.com/post/2020-02-11-svm-kernels/</link>
      <pubDate>Mon, 10 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/post/2020-02-11-svm-kernels/</guid>
      <description>


&lt;p&gt;In my effort to &lt;a href=&#34;https://www.jonesingfordata.com/post/2020-02-08-blogging-through-my-phd/&#34;&gt;blog my way through the rest of my PhD&lt;/a&gt; and &lt;a href=&#34;https://www.jonesingfordata.com/post/2020-02-08-comps-study-guide/&#34;&gt;study for comps&lt;/a&gt;, I present to you more on support vector machines.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;As I said &lt;a href=&#34;https://www.jonesingfordata.com/post/2020-02-09-review-of-svm-hastie/&#34;&gt;last time&lt;/a&gt;:
There are two components to SVMs.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The support vector classifier and&lt;/li&gt;
&lt;li&gt;Kernel methods&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first post covered the support vector classifier and its estimation.
In this post, I’m going to cover kernel methods for SVMs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-do-we-get-non-linearity-from-a-linear-classifier&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How do we get non-linearity from a linear classifier?&lt;/h2&gt;
&lt;p&gt;With SVMs, the intuition of addressing non-linearity is similar to linear regression.
Basically, you transform your right-hand-side variables in non-linear ways.
Remember that for linear regression, if my RHS variable is &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; but I want it to have a quadratic relationship to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, then I regress &lt;span class=&#34;math inline&#34;&gt;\(x^2\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; instead of trying to figure out some sort of quadratic regression.
The intuition with SVMs are the same: transform &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in a non-linear way, then use the linear support vector classifier on it.&lt;/p&gt;
&lt;p&gt;The intuition is the same, but of course it’s more complicated with SVMs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-1-transform-your-space-transform-your-life&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: transform your space transform your life&lt;/h2&gt;
&lt;p&gt;We want to project our features &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; from their original space, &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^p\)&lt;/span&gt;, to a new space, &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^q\)&lt;/span&gt;.
And perhaps &lt;span class=&#34;math inline&#34;&gt;\(q &amp;gt;&amp;gt; p\)&lt;/span&gt;.
We are going to do this with a function, &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;, such that &lt;span class=&#34;math inline&#34;&gt;\(h: \mathbb{R}^p \mapsto \mathbb{R}^q\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then, we fit a linear classifier on &lt;span class=&#34;math inline&#34;&gt;\(h(x)\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; like so:
&lt;span class=&#34;math inline&#34;&gt;\(f(x) = h(x)^T\beta + \beta_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Similar to the dual problem that we did in my last post we need to maximize&lt;/p&gt;
&lt;center&gt;
&lt;span class=&#34;math inline&#34;&gt;\(L_D = \sum_{i=1}^N\alpha_i - \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j y_iy_j \langle h(x_i), h(x_j) \rangle\)&lt;/span&gt;
&lt;/center&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\langle a, b \rangle\)&lt;/span&gt; represents the inner product between &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Using the result from last time that &lt;span class=&#34;math inline&#34;&gt;\(\beta = \sum_{i=1}^N\alpha_i y_i x_i\)&lt;/span&gt; and some substitution we see that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  f(x) &amp;amp;= h(x)^T\beta + \beta_0\\
       &amp;amp;= \sum_{i=1}^N \alpha_i y_i \langle h(x), h(x_i)\rangle + \beta_0
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and we can solve for &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; by solving &lt;span class=&#34;math inline&#34;&gt;\(y_i f(x_i) = 1\)&lt;/span&gt; for all of the &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \alpha_i &amp;lt; C\)&lt;/span&gt;, or for the support vectors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-what-is-a-kernel&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: what is a kernel?&lt;/h2&gt;
&lt;p&gt;We can see that to solve this linear classifier with the projected data &lt;span class=&#34;math inline&#34;&gt;\(h(x)\)&lt;/span&gt;, we need the inner products &lt;span class=&#34;math inline&#34;&gt;\(h(x)h(x)^T\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Support Vector Machines in Hastie et al.</title>
      <link>https://www.jonesingfordata.com/post/2020-02-09-review-of-svm-hastie/</link>
      <pubDate>Sun, 09 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/post/2020-02-09-review-of-svm-hastie/</guid>
      <description>


&lt;p&gt;In my effort to &lt;a href=&#34;https://www.jonesingfordata.com/post/2020-02-08-blogging-through-my-phd/&#34;&gt;blog my way through the rest of my PhD&lt;/a&gt; and &lt;a href=&#34;https://www.jonesingfordata.com/post/2020-02-08-comps-study-guide/&#34;&gt;study for comps&lt;/a&gt;, I present to you support vector machines. This is the first of at least 2 and possibly 3 articles on SVMs.&lt;/p&gt;
&lt;p&gt;When we covered SVMs in my ML class a couple years ago, we focused on computational methods rather than the math.
The focus for comps is more-or-less the opposite so we’re going with chapter 12 of &lt;a href=&#34;https://web.stanford.edu/~hastie/Papers/ESLII.pdf&#34;&gt;The Elements of Statistical Learning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’ve found that many academics in CS seem infatuated with SVMs and I’ve struggled to understand why.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Can anyone explain to me why a great many academic CS folks really really like SVMs?
&lt;/p&gt;
— Tommy Jones (&lt;span class=&#34;citation&#34;&gt;@thos_jones&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/thos_jones/status/1226581814959099905?ref_src=twsrc%5Etfw&#34;&gt;February 9, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;Thankfully, Twitter was there for me when I needed it most.
Apparently, the answer is “kernel methods plus the math is nice”.&lt;/p&gt;
&lt;p&gt;A warning for anyone reading this blog post: it’s probably terrible. :)
The introduction is probably a good way to think of SVMs.
Beyond that, I admit I dive into a lot of detail to help &lt;em&gt;me&lt;/em&gt; work through it.
But I’m not sure that it’s any more useful than just getting your own copy of The Elements of Statistical Learning.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;SVMs are one method to make a linear decision boundary for classification.
I hear you say “not all decision boundaries are linear”.
Well, dear reader, SVMs have an answer for you; read on.&lt;/p&gt;
&lt;p&gt;There are two components to SVMs.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The support vector classifier and&lt;/li&gt;
&lt;li&gt;Kernel methods&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The support vector classifier is what creates a linear boundary between classes.
(Or it creates a best guess at a linear boundary in the case of overlapping classes.)
I describe it in detail below.&lt;/p&gt;
&lt;p&gt;Kernel methods are the solution to the fact that not all data are linear.
Basically, a “kernel method” is a projection from one space to another.
The hope (plan? theory?) is that this new space will lead to a linear separation between classes.
In many (most?) cases, the new space will be of higher dimension.
This can give us two things:
First, as already stated, non-linear can become linear.
Second, when classes are overlapping, a higher dimension can give them more separability.
In the latter case, this could result in overfitting.
However, stick with me, dear reader, and we will see how SVMs address this issue.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-support-vector-classifier&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Support Vector Classifier&lt;/h2&gt;
&lt;p&gt;Remember this:
&lt;em&gt;The support vector classifier finds the linear hyperplane that separates classes with the maximum margin.&lt;/em&gt;
The image at the top shows this margin in the case of separable classes (left) and overlapping classes (right).&lt;/p&gt;
&lt;p&gt;Some definitions you’ll need to follow the math are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \text{outcomes: } &amp;amp; \{y: y_i \in \{-1,1\}\}\\
  \text{features: } &amp;amp; \{x: x_i \in \mathbb{R}^p\}\\
  \text{hyperplane: } &amp;amp; \{x: f(x) = x^T\beta + \beta_0\}\\
  \text{classification rule: } &amp;amp; G(x) = \text{sign}(x^T\beta + \beta_0)\\
  \text{margin: } &amp;amp; M = \frac{1}{\lVert\beta\rVert}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(\lVert \beta \rVert = 1\)&lt;/span&gt;, meaning that &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is a unit vector.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now we have two optimization problems to consider:
In the trivial case, the &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; are separable by class, &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;.
So then it’s just an issue of finding the “right” hyperplane.
In the more realistic case, the &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; are not completely separated by class.
This is a more difficult problem and requires a fancier solution.&lt;/p&gt;
&lt;div id=&#34;separable-classes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Separable classes&lt;/h3&gt;
&lt;p&gt;For the separable case, we have a basic optimization problem:&lt;/p&gt;
&lt;center&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\max_{\beta,\beta_0,\lVert\beta\rVert}M\)&lt;/span&gt; subject to &lt;span class=&#34;math inline&#34;&gt;\(y_i \cdot (x_i^T\beta + \beta_0) \geq M\)&lt;/span&gt;
&lt;/center&gt;
&lt;p&gt;According to Hastie et al. this can be rephrased and more easily solved by&lt;/p&gt;
&lt;center&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\min_{\beta,\beta_0}\lVert\beta\rVert\)&lt;/span&gt; subject to &lt;span class=&#34;math inline&#34;&gt;\(y_i\cdot(x^T\beta + \beta_0) \ge 1\)&lt;/span&gt;
&lt;/center&gt;
&lt;div id=&#34;a-total-aside-that-links-this-back-to-calculus&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;A total aside that links this back to calculus&lt;/h4&gt;
&lt;p&gt;You might not be able to solve this analytically, but if I recall my calculus, you’d solve&lt;/p&gt;
&lt;center&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\min_{\beta,\beta_0} \lVert\beta\rVert + \lambda(y_i\cdot(x^T\beta + \beta_0) - 1 - s^2)\)&lt;/span&gt;
&lt;/center&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(s^2\)&lt;/span&gt; is a variable introduced to handle inequalities a-la &lt;a href=&#34;http://users.wpi.edu/~pwdavis/Courses/MA1024B10/1024_Lagrange_multipliers.pdf&#34;&gt;here&lt;/a&gt;.
And &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is a Lagrange multiplier.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;non-separable-classes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Non-separable classes&lt;/h3&gt;
&lt;p&gt;When the classes aren’t separable, we have to introduce a new variable, called a “slack variable” &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt;.
(If—like me—you have trouble pronouncing &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Xi_(letter)&#34;&gt;it’s pronounced like “sigh”&lt;/a&gt;.)
&lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; is a vector the same length as &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; (and as many rows as &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;).
Using this variable allows some points to be on the wrong side of the margin.
(See the right image, above.)&lt;/p&gt;
&lt;p&gt;The standard way to modify the constraint in the face of a slack variable is this:&lt;/p&gt;
&lt;center&gt;
&lt;span class=&#34;math inline&#34;&gt;\(y_i \cdot (x^T\beta + \beta_0) \ge M\cdot(1 - \xi_i)\)&lt;/span&gt;
&lt;/center&gt;
&lt;p&gt;But we have to more constraints on the total number of misclassified observations.
The new constraints are&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \xi_i \ge 0 \forall i\\
  \sum_{i=1}^N \xi_i \leq K
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This leads to the way the support vector classifier is usually defined.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \min\lVert\beta\rVert &amp;amp;\text{  subject to  }
    \begin{cases}
      y_i(x_i^T\beta + \beta_0) \ge 1 - \xi_i \forall i,\\
      \xi_ \geq 0, \sum \xi_i \leq K
    \end{cases}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;another-calculus-linking-aside&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Another calculus linking aside&lt;/h4&gt;
&lt;p&gt;From the link above, if I wanted to do this with calculus I would have the following:&lt;/p&gt;
&lt;center&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\min_{\beta,\beta_0} \lVert\beta\rVert + \lambda_1(y_i\cdot(x^T\beta + \beta_0) - 1 - s_1^2) + \lambda_2(\xi - s_2^2) + \lambda_3(\sum_i\xi - K + s_3^2)\)&lt;/span&gt;
&lt;/center&gt;
&lt;p&gt;Full disclosure: I’m not 100% sure about the plus sign on &lt;span class=&#34;math inline&#34;&gt;\(s_3^2\)&lt;/span&gt;. Caveat emptor!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-it-the-way-hastie-et-al.do&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Solving it the way Hastie et al. do&lt;/h3&gt;
&lt;p&gt;Reader, I warn you that this section gets ugly and confusing.
Feel free to skip it unless you’re going to build your own support vector classifier from scratch.&lt;/p&gt;
&lt;p&gt;Hastie et al. (and I assume the rest of the ML world) rely on a couple of tricks to make the support vector classifier more computationally tractable.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;They restate the problem to something that makes the algebra a little nicer.&lt;/li&gt;
&lt;li&gt;They restate the problem again in a way that makes it nicer to put in a quadratic optimization solver.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To the second, some vocabulary:
The “primal” problem is the equation as originally stated.
The “dual” problem is an equivalent problem that, if solved, will result in the same answer.&lt;/p&gt;
&lt;p&gt;To make it more computationally friendly, the problem above is often re-stated as&lt;/p&gt;
&lt;center&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\min_{\beta,\beta_0} \frac{1}{2}\lVert\beta\rVert^2 + C\sum_{i=1}^N\xi_i\)&lt;/span&gt; subject to &lt;span class=&#34;math inline&#34;&gt;\(\xi_i \geq 0, y_i(x_i^T\beta + \beta_0) \geq 1 - \xi_i, \forall i\)&lt;/span&gt;
&lt;/center&gt;
&lt;p&gt;The Lagrange (primal) function becomes&lt;/p&gt;
&lt;center&gt;
&lt;span class=&#34;math inline&#34;&gt;\(L_P = \frac{1}{2}\lVert\beta\rVert^2 + C\sum_{i=1}^N\xi_i - \sum_{i=1}^N \alpha_i(y_i(x_i^T\beta+\beta_0) - (1 - \xi_i)) - \sum_{i=1}^N\mu_i\xi_i\)&lt;/span&gt;
&lt;/center&gt;
&lt;p&gt;I’m inferring that &lt;span class=&#34;math inline&#34;&gt;\(\alpha_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; are the Lagrange multipliers.
But maybe I’m wrong.
Caveat emptor again!&lt;/p&gt;
&lt;p&gt;Setting the derivatives equal to zero and doing some &lt;del&gt;magic&lt;/del&gt; math we see&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \beta &amp;amp;= \sum_{i=1}^N \alpha_i y_i x_i\\
  0 &amp;amp;= \sum_{i=1}^N \alpha_i y_i  \\
  \alpha_i &amp;amp;= C - \mu_i, \forall i
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which you can substitute back in to get the dual problem:&lt;/p&gt;
&lt;center&gt;
&lt;span class=&#34;math inline&#34;&gt;\(L_D = \sum_{i=1}^N\alpha_i - \frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_jy_iy_jx_i^Tx_j^T\)&lt;/span&gt;
&lt;/center&gt;
&lt;p&gt;The above is subject to several constraints.
The first couple are from our problem as we’ve already stated it:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  &amp;amp;0 \leq \alpha_i \leq C\\
  &amp;amp;\text{ and }\\
  &amp;amp;\sum_{i=1}^N\alpha_iy_i = 0
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And the remainder are from the &lt;a href=&#34;https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions&#34;&gt;Karush–Kuhn–Tucker conditions&lt;/a&gt;.
(I’m going to refer to them as the KKT conditions.)
The KKT conditions help with finding solutions to nonlinear optimization problems.
The additional constraints that they introduce are&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \alpha_i(y_i(x_i^T\beta + \beta_0) - (1 - \xi_i)) &amp;amp;= 0\\
  \mu_i\xi_i &amp;amp;= 0\\
  y_i(x_i^T\beta+\beta_0) - (1 - \xi_i) &amp;amp;\geq 0
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bringing-it-home&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bringing it home&lt;/h3&gt;
&lt;p&gt;Ignoring the computational details that we just painfully went through (and still weren’t enough to get you to actually compute it yourself):&lt;/p&gt;
&lt;p&gt;The solution for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; has the form &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta = \sum_{i=1}^N \hat\alpha_i y_i x_x\)&lt;/span&gt;.
For the overwhelming majority of observations, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\hat\alpha_i = 0\)&lt;/span&gt;.
The ones where &lt;span class=&#34;math inline&#34;&gt;\(\hat\alpha_i \neq 0\)&lt;/span&gt; are due to the case where &lt;span class=&#34;math inline&#34;&gt;\((x_i^T\beta + \beta_0) - (1 - \xi_i)) = 0\)&lt;/span&gt; exactly.
(Note the first and last of the KKT conditions.)&lt;/p&gt;
&lt;p&gt;These points are called “support vectors” since &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta\)&lt;/span&gt; is represented by them alone.
Of those, some points lie exactly on the margin.
In that case &lt;span class=&#34;math inline&#34;&gt;\(\hat\xi_i=0\)&lt;/span&gt; and consequently &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \hat\alpha_i &amp;lt; C\)&lt;/span&gt;.
We use these points to solve for &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, usually by averaging across them.
For the remainder &lt;span class=&#34;math inline&#34;&gt;\(\hat\xi_i&amp;gt;0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat\alpha_i = C\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, as indicated way up at the top, you need to use the sign of &lt;span class=&#34;math inline&#34;&gt;\(x_i^T\hat\beta + \hat\beta_0\)&lt;/span&gt; to make a class assignment.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;next-time&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Next time&lt;/h2&gt;
&lt;p&gt;Next time I’ll write about kernel methods for SVMs and how we use an extension of the support vector classifier to estimate SVMs.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I have two questions about this: 1. Why? 2. Does this include &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;? It’s always seemed more elegant to me when writing linear equations to just do &lt;span class=&#34;math inline&#34;&gt;\(x^T\beta\)&lt;/span&gt; where one vector of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is ones. For linear regression, that has no consequences, but it might here (or for ridge or lasso regressions).&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Blogging my way through the rest of my PhD</title>
      <link>https://www.jonesingfordata.com/post/2020-02-08-blogging-through-my-phd/</link>
      <pubDate>Sat, 08 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/post/2020-02-08-blogging-through-my-phd/</guid>
      <description>


&lt;p&gt;I’ve reached a turning point in my PhD studies.
Classes are behind me and what lies ahead is largely unstructured. Success or failure will largely be a function of whether or not (and how well) I pull myself together.&lt;/p&gt;
&lt;p&gt;The next steps are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Pass comprehensive exams AKA “comps” (scheduled for early April 2020)&lt;/li&gt;
&lt;li&gt;Propose and defend a dissertation topic (which must be completed by spring of 2021)&lt;/li&gt;
&lt;li&gt;Write and defend the dissertation itself&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Everyone knows that dissertation research is unstructured.
The lack of structure is—from what I can tell—a large source of struggle for PhD students.
(Another, unfortunately, is lack of support from their advisors and program.
Thankfully, I’ve been blessed with an amazing mentor and advisor in &lt;a href=&#34;https://cos.gmu.edu/cds/faculty-profile-william-kennedy/.&#34;&gt;Bill Kennedy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yet in my program—&lt;a href=&#34;https://cos.gmu.edu/cds/phd-in-computational-sciences-and-informatics/&#34;&gt;Computational Sciences and Informatics (CSI) at George Mason University&lt;/a&gt;—even comps are unstructured.
The program of study is largely a “choose your own adventure” degree.
As a result, there is no set exam that every student takes. Instead you form a committee as you finish classes and then more-or-less negotiate the subjects of your exam with your committee members.
There is some structure, but the content itself is bespoke based on courses you’ve taken and your intended dissertation focus.&lt;/p&gt;
&lt;p&gt;I have been taught (and reinforced by experience) that you don’t just write to explain; you write to understand.
To that end, I intend to start blogging again, but this time with focus.
My goal is to blog about what I’m studying for comps, what I’m reading related to my research, and what I’m thinking about.
(That last bit is to help articulate and codify some of the ideas knocking around in my head.)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://media.giphy.com/media/WOYKaXG2xJsBO/giphy.gif&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://media.giphy.com/media/WOYKaXG2xJsBO/giphy.gif&#34; width=&#34;50%&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Comprehensive exam study guide</title>
      <link>https://www.jonesingfordata.com/post/2020-02-08-comps-study-guide/</link>
      <pubDate>Sat, 08 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/post/2020-02-08-comps-study-guide/</guid>
      <description>


&lt;p&gt;This post is going to make a boring read.&lt;/p&gt;
&lt;p&gt;As I explained &lt;a href=&#34;https://www.jonesingfordata.com/post/2020-02-08-blogging-through-my-phd/&#34;&gt;in my last post&lt;/a&gt;, my program’s comprehensive exams are bespoke for the student.
I’m going to use this post as a high-level study guide to keep me on track.
I’ll update it as I get more clarification etc.&lt;/p&gt;
&lt;p&gt;The basic structure of the exam is as follows:&lt;/p&gt;
&lt;p&gt;There is a one-day (up to six-hour) written exam taken in-person at the university and proctored by one committee member.
The results are classified as “Pass”, “Fail”, or “Oral Exam Needed”.
The latter is used if I need to clarify or expand on my written answer.
The exam questions are submitted by 3 of my 5 committee members.
The questions are designed to track courses that I took as part of my degree “concentration” courses.&lt;/p&gt;
&lt;p&gt;After the written portion, there is a computational exam which I will have two weeks to complete.
Two of my committee members are taking on putting together this portion of the exam.&lt;/p&gt;
&lt;p&gt;Finally, I will convene in-person with my committee and (a) present on the results of the computational portion of the exam and (b) undergo any oral examination needed from the answers to my written questions.&lt;/p&gt;
&lt;p&gt;As a personal aside/musing: looking at what I’m being tested on, it feels like my degree concentration is more “machine learning” than “computational statistics”. Though, TBH these days, I feel like that’s more of a cultural statement than a mathematical one. I still self identify as a statistician and regularly attend JSM. ¯\&lt;em&gt;(ツ)&lt;/em&gt;/¯&lt;/p&gt;
&lt;div id=&#34;the-written-portion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The written portion&lt;/h2&gt;
&lt;p&gt;The courses we chose to base the written portion of the exam were&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://seor.vse.gmu.edu/~klaskey/SYST664/SYST664.html&#34;&gt;Bayesian Inference and Decision Theory&lt;/a&gt; which I actually took at &lt;a href=&#34;https://myaccess.georgetown.edu/pls/bninbp/bwckctlg.p_disp_course_detail?cat_term_in=201810&amp;amp;subj_code_in=MATH&amp;amp;crse_numb_in=640&#34;&gt;Georgetown&lt;/a&gt; through &lt;a href=&#34;https://www.consortium.org/&#34;&gt;the Consortium of Universities in the Washington Metropolitan Area&lt;/a&gt;, which lets me take comparable courses at any university with less bureaucracy than using transfer credits.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://patriotweb.gmu.edu:9977/pls/prod/bwckctlg.p_disp_course_detail?cat_term_in=201670&amp;amp;subj_code_in=CSI&amp;amp;crse_numb_in=777&#34;&gt;Principles of Knowledge Mining&lt;/a&gt; which is really a machine learning course but focused on data mining&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://math.gmu.edu/syllabi/F17/689-001-Griva.pdf&#34;&gt;Computational Learning and Discovery&lt;/a&gt; which is also a machine learning course but focused more on the mathematics of the various methods&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since Principles of Knowledge Mining and Computational Learning and Discovery had significant overlap in material we decided to split the exam based on supervised learning and unsupervised learning, without distinction of which course it came from.&lt;/p&gt;
&lt;div id=&#34;bayesian-stats&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bayesian stats&lt;/h3&gt;
&lt;p&gt;Technically, my main study resource for this will be the notes, homework, and exams from my Bayesian stats class. (This is an advantage of having the instructor for the course on your committee.) However, two books that I may use as additional reference are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/&#34;&gt;Bayesian Data Analysis&lt;/a&gt; by Gelman et al.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://link.springer.com/book/10.1007/978-0-387-92407-6&#34;&gt;A First Course in Bayesian Statistical Methods&lt;/a&gt; by Hoff&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And I’ve been told that everything in the course is fair game. That said, I’m going to focus on the areas that feel the most rusty to me, namely&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Metropolis Hastings (this still seems like magic to me)&lt;/li&gt;
&lt;li&gt;Gibbs sampling&lt;/li&gt;
&lt;li&gt;Model checking and evaluation&lt;/li&gt;
&lt;li&gt;Bayesian regression - linear and ridge regression&lt;/li&gt;
&lt;li&gt;The Dirichlet multinomial&lt;/li&gt;
&lt;li&gt;The Dirichlet multinomial with a hierarchical uniform prior&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;WRT that last two, I definitely know they’re going to come into play during my dissertation itself.
I’m planning to (a) implement a NUTS sampler for LDA as a derivative of MH sampler in the WarpLDA algorithm and (b) would like to implement an LDA derivative that puts a hierarchical uniform prior on &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\alpha\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;supervised-learning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Supervised learning&lt;/h3&gt;
&lt;p&gt;This section is still a bit “TBD” but what I know for sure follows:&lt;/p&gt;
&lt;p&gt;The books I’m using here are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/~hastie/Papers/ESLII.pdf&#34;&gt;The Elements of Statistical Learning&lt;/a&gt; by Hastie et al.&lt;/li&gt;
&lt;li&gt;TBD likely either Tan et al. (see below) or &lt;a href=&#34;https://www.cs.waikato.ac.nz/ml/weka/book.html&#34;&gt;Data Mining&lt;/a&gt; by Witten et al.&lt;/li&gt;
&lt;li&gt;I likely won’t be using &lt;a href=&#34;https://www.deeplearningbook.org/&#34;&gt;Deep Learning&lt;/a&gt; by Goodfellow et al. but in case someone comes across this page as a resource to study ML themselves, I feel I’d be remiss not to mention it. This is an excellent book for those of us with a more mathematical bent.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The topics I’ll be focusing on are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;SVMs from Hastie et. al (Ch. 12)&lt;/li&gt;
&lt;li&gt;TBD&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;unsupervised-learning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Unsupervised learning&lt;/h3&gt;
&lt;p&gt;The books I’m using here are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/~hastie/Papers/ESLII.pdf&#34;&gt;The Elements of Statistical Learning&lt;/a&gt; by Hastie et al.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www-users.cs.umn.edu/~kumar001/dmbook/index.php&#34;&gt;Introduction to Data Mining&lt;/a&gt; by Tan et al.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The topics I’ll be focusing on are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Clustering from Hastie et al. (Ch. 14, specifically section 14.3)&lt;/li&gt;
&lt;li&gt;Clustering from Tan et al. (Ch. 8 - 9 in 1st ed. TBD on whether or not I do 1st or 2nd ed.)&lt;/li&gt;
&lt;li&gt;Anomaly detection from Tan et al. (Ch. 10 in 1st ed.)&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;clustering&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Clustering&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;K-means &amp;amp; K-medoids&lt;/li&gt;
&lt;li&gt;Agglomorative hierarchical clustering&lt;/li&gt;
&lt;li&gt;DBSCAN&lt;/li&gt;
&lt;li&gt;Gaussian mixture model&lt;/li&gt;
&lt;li&gt;Other clustering algorithms
&lt;ul&gt;
&lt;li&gt;Prototype-based clustering&lt;/li&gt;
&lt;li&gt;Density-based clustering (non-DBSCAN)&lt;/li&gt;
&lt;li&gt;Graph-based clustering (including page-rank)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cluster evaluation&lt;/li&gt;
&lt;li&gt;Which clustering algorithm?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;dimensionality-reduction&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Dimensionality reduction&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Principal component analysis&lt;/li&gt;
&lt;li&gt;Non-negative matrix factorization&lt;/li&gt;
&lt;li&gt;Independent component analysis&lt;/li&gt;
&lt;li&gt;Multidimensional scaling&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;anomaly-detection&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Anomaly detection&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Statistical approaches&lt;/li&gt;
&lt;li&gt;Proximity-based outlier detection&lt;/li&gt;
&lt;li&gt;Density-based outlier detection&lt;/li&gt;
&lt;li&gt;Clustering-based techniques&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;other-topics-in-machine-learning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other topics in machine learning&lt;/h3&gt;
&lt;p&gt;Other topics will be&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Overfitting&lt;/li&gt;
&lt;li&gt;Generalization&lt;/li&gt;
&lt;li&gt;Bias/variance tradeoff&lt;/li&gt;
&lt;li&gt;Model selection&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These are covered Hastie et al. Ch. 7.&lt;/p&gt;
&lt;p&gt;I am encouraged to look into the introductory chapters of &lt;a href=&#34;http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf&#34;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-computational-portion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The computational portion&lt;/h2&gt;
&lt;p&gt;I don’t really need to “study” for the computation portion.
From discussions, what we are going to do is do some flag planting for a paper that I intend to write as part of the dissertation.
I’ve implemented a couple forms of transfer learning for LDA (or an LDA-like model) in a &lt;a href=&#34;https://github.com/tommyjones/tidylda&#34;&gt;new R package I’m working on&lt;/a&gt;.
So we’d be looking at a preliminary study of that.
Thing is, we don’t know how permanent/forgetful LDA is in this paradigm.
And I don’t know that the way I implemented it is optimal.
Topics to consider might be&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Weighting/reweighting of the previous model’s topics in the prior of a new model.
This tunes how much the new model “remembers” the old model.&lt;/li&gt;
&lt;li&gt;Initialization strategies.
Algorithms for LDA shuffle around counts of document-token-topic assignments. If you want to “transfer” you should initialize your counts in proportion to that of the previously-trained model. Unfortunately, corpora don’t have the same number of tokens overall or per-document. So, how do you initialize those counts then? (FWIW, I am not convinced that the current way I did it in my in-development R package is the best strategy.)&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://www.jonesingfordata.com/about/</link>
      <pubDate>Sun, 10 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/about/</guid>
      <description>&lt;p&gt;My name is Tommy. I&amp;rsquo;m a member of the technical staff at In-Q-Tel and a coordinator for Data Science DC. I have an MS in mathematics and statistics from Georgetown University and a BA in economics from the College of William and Mary. I am also a PhD student in the George Mason University Department of Computational and Data Sciences. I am the author of the textmineR package for the R language. I am also a Marine Corps veteran. Terrible opinions are my own.&lt;/p&gt;

&lt;p&gt;Check out my &lt;a href=&#34;https://github.com/tommyjones&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can follow me on twitter &lt;a href=&#34;https://twitter.com/thos_jones&#34; target=&#34;_blank&#34;&gt;\@thos_jones&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optimizing Topic Models for Classification Tasks</title>
      <link>https://www.jonesingfordata.com/talk/2019_11_09_dcr/</link>
      <pubDate>Sat, 09 Nov 2019 10:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/talk/2019_11_09_dcr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Brief Introduction to Graph Theory</title>
      <link>https://www.jonesingfordata.com/talk/2019_04_15_dsdc/</link>
      <pubDate>Mon, 15 Apr 2019 18:30:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/talk/2019_04_15_dsdc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>https://www.jonesingfordata.com/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/publication/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://www.jonesingfordata.com/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/slides/example/</guid>
      <description>

&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;

&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Code block:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;

&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;

&lt;p&gt;Block math:&lt;/p&gt;

&lt;p&gt;$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;

&lt;p&gt;Make content appear incrementally&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
   One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three
&lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;

&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;


&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;


&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;

&lt;p&gt;Customize the slide style and background&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;

&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mining Texts with textmineR</title>
      <link>https://www.jonesingfordata.com/talk/2018_11_09_dcr/</link>
      <pubDate>Fri, 09 Nov 2018 10:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/talk/2018_11_09_dcr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>textmineR - NLP with R</title>
      <link>https://www.jonesingfordata.com/talk/2016_04_27_spdc/</link>
      <pubDate>Sun, 17 Apr 2016 18:30:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/talk/2016_04_27_spdc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>https://www.jonesingfordata.com/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/publication/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Topic Modeling with LDA and more</title>
      <link>https://www.jonesingfordata.com/talk/2014_11_12_dcnlp/</link>
      <pubDate>Wed, 12 Nov 2014 18:30:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/talk/2014_11_12_dcnlp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>https://www.jonesingfordata.com/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>marginal</title>
      <link>https://www.jonesingfordata.com/project/marginal/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/project/marginal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>mvrsquared</title>
      <link>https://www.jonesingfordata.com/project/mvrsquared/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/project/mvrsquared/</guid>
      <description></description>
    </item>
    
    <item>
      <title>R-squared for Topic Models</title>
      <link>https://www.jonesingfordata.com/project/rsquared/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/project/rsquared/</guid>
      <description></description>
    </item>
    
    <item>
      <title>textmineR</title>
      <link>https://www.jonesingfordata.com/project/textminer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/project/textminer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>tidylda</title>
      <link>https://www.jonesingfordata.com/project/tidylda/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.jonesingfordata.com/project/tidylda/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
