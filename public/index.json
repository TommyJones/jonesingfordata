[{"authors":["tommy"],"categories":null,"content":"My name is Tommy. I\u0026rsquo;m a member of the technical staff at In-Q-Tel and a coordinator for Data Science DC. I have an MS in mathematics and statistics from Georgetown University and a BA in economics from the College of William and Mary. I am also a PhD student in the George Mason University Department of Computational and Data Sciences. I am the author of the textmineR package for the R language. I am also a Marine Corps veteran. My opinions are terrible and they are all my own.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.jonesingfordata.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"My name is Tommy. I\u0026rsquo;m a member of the technical staff at In-Q-Tel and a coordinator for Data Science DC. I have an MS in mathematics and statistics from Georgetown University and a BA in economics from the College of William and Mary. I am also a PhD student in the George Mason University Department of Computational and Data Sciences. I am the author of the textmineR package for the R language.","tags":null,"title":"Tommy Jones","type":"authors"},{"authors":[],"categories":[],"content":" In my effort to blog my way through the rest of my PhD and study for comps, I present to you support vector machines.\nWhen we covered SVMs in my ML class a couple years ago, we focused on computational methods rather than the math. The focus for comps is more-or-less the opposite so we’re going with chapter 12 of The Elements of Statistical Learning.\nI’ve found that many academics in CS seem infatuated with SVMs and I’ve struggled to understand why.\nCan anyone explain to me why a great many academic CS folks really really like SVMs? — Tommy Jones (@thos_jones) February 9, 2020   Thankfully, Twitter was there for me when I needed it most. Apparently, the answer is “kernel methods plus the math is nice”.\nIntroduction SVMs are one method to make a linear decision boundary for classification. I hear you say “not all decision boundaries are linear”. Well, dear reader, SVMs have an answer for you; read on.\nThere are two components to SVMs.\nThe support vector classifier and Kernel methods  The support vector classifier is what creates a linear boundary between classes. (Or it creates a best guess at a linear boundary in the case of overlapping classes.) I describe it in detail below.\nKernel methods are the solution to the fact that not all data are linear. Basically, a “kernel method” is a projection from one space to another. The hope (plan? theory?) is that this new space will lead to a linear separation between classes. In many (most?) cases, the new space will be of higher dimension. This can give us two things: First, as already stated, non-linear can become linear. Second, when classes are overlapping, a higher dimension can give them more separability. In the latter case, this could result in overfitting. However, stick with me, dear reader, and we will see how SVMs address this issue.\n The Support Vector Classifier Remember this: The support vector classifier finds the linear hyperplane that separates classes with the maximum margin. The image at the top shows this margin in the case of separable classes (left) and overlapping classes (right).\nSome definitions you’ll need to follow the math are:\n\\[\\begin{align} \\text{outcomes: } \u0026amp; \\{y: y_i \\in \\{-1,1\\}\\}\\\\ \\text{features: } \u0026amp; \\{x: x_i \\in \\mathbb{R}^p\\}\\\\ \\text{hyperplane: } \u0026amp; \\{x: f(x) = x^T\\beta + \\beta_0\\}\\\\ \\text{classification rule: } \u0026amp; G(x) = \\text{sign}(x^T\\beta + \\beta_0)\\\\ \\text{margin: } \u0026amp; M = \\frac{1}{\\lVert\\beta\\rVert} \\end{align}\\]\nNote that \\(\\lVert \\beta \\rVert = 1\\), meaning that \\(\\beta\\) is a unit vector.1\nNow we have two optimization problems to consider: In the trivial case, the \\(x_i\\) are separable by class, \\(y_i\\). So then it’s just an issue of finding the “right” hyperplane. In the more realistic case, the \\(x_i\\) are not completely separated by class. This is a more difficult problem and requires a fancier solution.\nSeparable classes For the separable case, we have a basic optimization problem:\n \\(\\max_{\\beta,\\beta_0,\\lVert\\beta\\rVert}M\\) subject to \\(y_i \\cdot (x_i^T\\beta + \\beta_0) \\geq M\\)  According to Hastie et al. this can be rephrased and more easily solved by\n \\(\\min_{\\beta,\\beta_0}\\lVert\\beta\\rVert\\) subject to \\(y_i\\cdot(x^T\\beta + \\beta_0) \\ge 1\\)  A total aside that links this back to calculus You might not be able to solve this analytically, but if I recall my calculus, you’d solve\n \\(\\min_{\\beta,\\beta_0} \\lVert\\beta\\rVert + \\lambda(y_i\\cdot(x^T\\beta + \\beta_0) - 1 - s^2)\\)  Where \\(s^2\\) is a variable introduced to handle inequalities a-la here. And \\(\\lambda\\) is a Lagrange multiplier.\n  Non-separable classes When the classes aren’t separable, we have to introduce a new variable, called a “slack variable” \\(\\xi\\). (If—like me—you have trouble pronouncing \\(\\xi\\), it’s pronounced like “sigh”.) \\(\\xi\\) is a vector the same length as \\(y\\) (and as many rows as \\(x\\)). Using this variable allows some points to be on the wrong side of the margin. (See the right image, above.)\nThe standard way to modify the constraint in the face of a slack variable is this:\n \\(y_i \\cdot (x^T\\beta + \\beta_0) \\ge M\\cdot(1 - \\xi_i)\\)  But we have to more constraints on the total number of misclassified observations. The new constraints are\n\\[\\begin{align} \\xi_i \\ge 0 \\forall i\\\\ \\sum_{i=1}^N \\xi_i \\leq K \\end{align}\\]\nThis leads to the way the support vector classifier is usually defined.\n\\[\\begin{align} \\min\\lVert\\beta\\rVert \u0026amp;\\text{ subject to } \\begin{cases} y_i(x_i^T\\beta + \\beta_0) \\ge 1 - \\xi_i \\forall i,\\\\ \\xi_ \\geq 0, \\sum \\xi_i \\leq K \\end{cases} \\end{align}\\]\nAnother calculus linking aside From the link above, if I wanted to do this with calculus I would have the following:\n \\(\\min_{\\beta,\\beta_0} \\lVert\\beta\\rVert + \\lambda_1(y_i\\cdot(x^T\\beta + \\beta_0) - 1 - s_1^2) + \\lambda_2(\\xi - s_2^2) + \\lambda_3(\\sum_i\\xi - K + s_3^2)\\)  Full disclosure: I’m not 100% sure about the plus sign on \\(s_3^2\\). Caveat emptor!\n  Solving it the way Hastie et al. do Reader, I warn you that this section gets ugly and confusing. Feel free to skip it unless you’re going to build your own support vector classifier from scratch.\nHastie et al. (and I assume the rest of the ML world) rely on a couple of tricks to make the support vector classifier more computationally tractable.\nThey restate the problem to something that makes the algebra a little nicer. They restate the problem again in a way that makes it nicer to put in a quadratic optimization solver.  To the second, some vocabulary: The “primal” problem is the equation as originally stated. The “dual” problem is an equivalent problem that, if solved, will result in the same answer.\nTo make it more computationally friendly, the problem above is often re-stated as\n \\(\\min_{\\beta,\\beta_0} \\frac{1}{2}\\lVert\\beta\\rVert^2 + C\\sum_{i=1}^N\\xi_i\\) subject to \\(\\xi_i \\geq 0, y_i(x_i^T\\beta + \\beta_0) \\geq 1 - \\xi_i, \\forall i\\)  The Lagrange (primal) function becomes\n \\(L_P = \\frac{1}{2}\\lVert\\beta\\rVert^2 + C\\sum_{i=1}^N\\xi_i - \\sum_{i=1}^N \\alpha_i(y_i(x_i^T\\beta+\\beta_0) - (1 - \\xi_i)) - \\sum_{i=1}^N\\mu_i\\xi_i\\)  I’m inferring that \\(\\alpha_i\\) and \\(\\mu_i\\) are the Lagrange multipliers. But maybe I’m wrong. Caveat emptor again!\nSetting the derivatives equal to zero and doing some magic math we see\n\\[\\begin{align} \\beta \u0026amp;= \\sum_{i=1}^N \\alpha_i y_i x_i\\\\ 0 \u0026amp;= \\sum_{i=1}^N \\alpha_i y_i \\\\ \\alpha_i \u0026amp;= C - \\mu_i, \\forall i \\end{align}\\]\nwhich you can substitute back in to get the dual problem:\n \\(L_D = \\sum_{i=1}^N\\alpha_i - \\frac{1}{2} \\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i\\alpha_jy_iy_jx_i^Tx_j^T\\)  The above is subject to several constraints. The first couple are from our problem as we’ve already stated it:\n\\[\\begin{align} \u0026amp;0 \\leq \\alpha_i \\leq C\\\\ \u0026amp;\\text{ and }\\\\ \u0026amp;\\sum_{i=1}^N\\alpha_iy_i = 0 \\end{align}\\]\nAnd the remainder are from the Karush–Kuhn–Tucker conditions. (I’m going to refer to them as the KKT conditions.) The KKT conditions help with finding solutions to nonlinear optimization problems. The additional constraints that they introduce are\n\\[\\begin{align} \\alpha_i(y_i(x_i^T\\beta + \\beta_0) - (1 - \\xi_i)) \u0026amp;= 0\\\\ \\mu_i\\xi_i \u0026amp;= 0\\\\ y_i(x_i^T\\beta+\\beta_0) - (1 - \\xi_i) \u0026amp;\\geq 0 \\end{align}\\]\n Bringing it home Ignoring the computational details that we just painfully went through (and still weren’t enough to get you to actually compute it yourself):\nThe solution for \\(\\beta\\) has the form \\(\\hat\\beta = \\sum_{i=1}^N \\hat\\alpha_i y_i x_x\\). For the overwhelming majority of observations, \\(i\\), \\(\\hat\\alpha_i = 0\\). The ones where \\(\\hat\\alpha_i \\neq 0\\) are due to the case where \\((x_i^T\\beta + \\beta_0) - (1 - \\xi_i)) = 0\\) exactly. (Note the first and last of the KKT conditions.)\nThese points are called “support vectors” since \\(\\hat\\beta\\) is represented by them alone. Of those, some points lie exactly on the margin. In that case \\(\\hat\\xi_i=0\\) and consequently \\(0 \u0026lt; \\hat\\alpha_i \u0026lt; C\\). We use these points to solve for \\(\\beta_0\\), usually by averaging across them. For the remainder \\(\\hat\\xi_i\u0026gt;0\\) and \\(\\hat\\alpha_i = C\\).\nFinally, as indicated way up at the top, you need to use the sign of \\(x_i^T\\hat\\beta + \\hat\\beta_0\\) to make a class assignment.\n  Next time Next time I’ll write about kernel methods for SVMs and how we use an extension of the support vector classifier to estimate SVMs.\n  I have two questions about this: 1. Why? 2. Does this include \\(\\beta_0\\)? It’s always seemed more elegant to me when writing linear equations to just do \\(x^T\\beta\\) where one vector of \\(x\\) is ones. For linear regression, that has no consequences, but it might here (or for ridge or lasso regressions).↩\n   ","date":1581206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581292800,"objectID":"5b92a21603a91bb387c0f28388c83d31","permalink":"https://www.jonesingfordata.com/post/2020-02-09-review-of-svm-hastie/","publishdate":"2020-02-09T00:00:00Z","relpermalink":"/post/2020-02-09-review-of-svm-hastie/","section":"post","summary":"In my effort to blog my way through the rest of my PhD and study for comps, I present to you support vector machines.\nWhen we covered SVMs in my ML class a couple years ago, we focused on computational methods rather than the math. The focus for comps is more-or-less the opposite so we’re going with chapter 12 of The Elements of Statistical Learning.\nI’ve found that many academics in CS seem infatuated with SVMs and I’ve struggled to understand why.","tags":[],"title":"Support Vector Machines in Hastie et al.","type":"post"},{"authors":[],"categories":[],"content":" I’ve reached a turning point in my PhD studies. Classes are behind me and what lies ahead is largely unstructured. Success or failure will largely be a function of whether or not (and how well) I pull myself together.\nThe next steps are:\nPass comprehensive exams AKA “comps” (scheduled for early April 2020) Propose and defend a dissertation topic (which must be completed by spring of 2021) Write and defend the dissertation itself  Everyone knows that dissertation research is unstructured. The lack of structure is—from what I can tell—a large source of struggle for PhD students. (Another, unfortunately, is lack of support from their advisors and program. Thankfully, I’ve been blessed with an amazing mentor and advisor in Bill Kennedy\nYet in my program—Computational Sciences and Informatics (CSI) at George Mason University—even comps are unstructured. The program of study is largely a “choose your own adventure” degree. As a result, there is no set exam that every student takes. Instead you form a committee as you finish classes and then more-or-less negotiate the subjects of your exam with your committee members. There is some structure, but the content itself is bespoke based on courses you’ve taken and your intended dissertation focus.\nI have been taught (and reinforced by experience) that you don’t just write to explain; you write to understand. To that end, I intend to start blogging again, but this time with focus. My goal is to blog about what I’m studying for comps, what I’m reading related to my research, and what I’m thinking about. (That last bit is to help articulate and codify some of the ideas knocking around in my head.)\n\n","date":1581120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581292800,"objectID":"645d965bdd84a7f6338cb3b5455d50a8","permalink":"https://www.jonesingfordata.com/post/2020-02-08-blogging-through-my-phd/","publishdate":"2020-02-08T00:00:00Z","relpermalink":"/post/2020-02-08-blogging-through-my-phd/","section":"post","summary":"I’ve reached a turning point in my PhD studies. Classes are behind me and what lies ahead is largely unstructured. Success or failure will largely be a function of whether or not (and how well) I pull myself together.\nThe next steps are:\nPass comprehensive exams AKA “comps” (scheduled for early April 2020) Propose and defend a dissertation topic (which must be completed by spring of 2021) Write and defend the dissertation itself  Everyone knows that dissertation research is unstructured.","tags":[],"title":"Blogging my way through the rest of my PhD","type":"post"},{"authors":[],"categories":[],"content":" This post is going to make a boring read.\nAs I explained in my last post, my program’s comprehensive exams are bespoke for the student. I’m going to use this post as a high-level study guide to keep me on track. I’ll update it as I get more clarification etc.\nThe basic structure of the exam is as follows:\nThere is a one-day (up to six-hour) written exam taken in-person at the university and proctored by one committee member. The results are classified as “Pass”, “Fail”, or “Oral Exam Needed”. The latter is used if I need to clarify or expand on my written answer. The exam questions are submitted by 3 of my 5 committee members. The questions are designed to track courses that I took as part of my degree “concentration” courses.\nAfter the written portion, there is a computational exam which I will have two weeks to complete. Two of my committee members are taking on putting together this portion of the exam.\nFinally, I will convene in-person with my committee and (a) present on the results of the computational portion of the exam and (b) undergo any oral examination needed from the answers to my written questions.\nAs a personal aside/musing: looking at what I’m being tested on, it feels like my degree concentration is more “machine learning” than “computational statistics”. Though, TBH these days, I feel like that’s more of a cultural statement than a mathematical one. I still self identify as a statistician and regularly attend JSM. ¯\\(ツ)/¯\nThe written portion The courses we chose to base the written portion of the exam were\nBayesian Inference and Decision Theory which I actually took at Georgetown through the Consortium of Universities in the Washington Metropolitan Area, which lets me take comparable courses at any university with less bureaucracy than using transfer credits. Principles of Knowledge Mining which is really a machine learning course but focused on data mining Computational Learning and Discovery which is also a machine learning course but focused more on the mathematics of the various methods  Since Principles of Knowledge Mining and Computational Learning and Discovery had significant overlap in material we decided to split the exam based on supervised learning and unsupervised learning, without distinction of which course it came from.\nBayesian stats Technically, my main study resource for this will be the notes, homework, and exams from my Bayesian stats class. (This is an advantage of having the instructor for the course on your committee.) However, two books that I may use as additional reference are\nBayesian Data Analysis by Gelman et al. A First Course in Bayesian Statistical Methods by Hoff  And I’ve been told that everything in the course is fair game. That said, I’m going to focus on the areas that feel the most rusty to me, namely\nMetropolis Hastings (this still seems like magic to me) Model checking and evaluation Bayesian regression - linear and ridge regression The Dirichlet multinomial The Dirichlet multinomial with a hierarchical uniform prior  WRT that last two, I definitely know they’re going to come into play during my dissertation itself. I’m planning to (a) implement a NUTS sampler for LDA as a derivative of MH sampler in the WarpLDA algorithm and (b) would like to implement an LDA derivative that puts a hierarchical uniform prior on \\(\\boldsymbol\\alpha\\).\n Supervised learning This section is still a bit “TBD” but what I know for sure follows:\nThe books I’m using here are\nThe Elements of Statistical Learning by Hastie et al. TBD likely either Tan et al. (see below) or Data Mining by Witten et al. I likely won’t be using Deep Learning by Goodfellow et al. but in case someone comes across this page as a resource to study ML themselves, I feel I’d be remiss not to mention it. This is an excellent book for those of us with a more mathematical bent.  The topics I’ll be focusing on are\nSVMs from Hastie et. al (Ch. 12) TBD   Unsupervised learning The books I’m using here are\nThe Elements of Statistical Learning by Hastie et al. Introduction to Data Mining by Tan et al.  The topics I’ll be focusing on are\nClustering from Hastie et al. (Ch. 14, specifically section 14.3) Clustering from Tan et al. (Ch. 8 - 9 in 1st ed. TBD on whether or not I do 1st or 2nd ed.) Anomaly detection from Tan et al. (Ch. 10 in 1st ed.)   Other topics in machine learning Other topics will be\nOverfitting Generalization Bias/variance tradeoff Model selection  These are covered Hastie et al. Ch. 7.\nI am encouraged to look into the introductory chapters of Pattern Recognition and Machine Learning\n  The computational portion I don’t really need to “study” for the computation portion. From discussions, what we are going to do is do some flag planting for a paper that I intend to write as part of the dissertation. I’ve implemented a couple forms of transfer learning for LDA (or an LDA-like model) in a new R package I’m working on. So we’d be looking at a preliminary study of that. Thing is, we don’t know how permanent/forgetful LDA is in this paradigm. And I don’t know that the way I implemented it is optimal. Topics to consider might be\nWeighting/reweighting of the previous model’s topics in the prior of a new model. This tunes how much the new model “remembers” the old model. Initialization strategies. Algorithms for LDA shuffle around counts of document-token-topic assignments. If you want to “transfer” you should initialize your counts in proportion to that of the previously-trained model. Unfortunately, corpora don’t have the same number of tokens overall or per-document. So, how do you initialize those counts then? (FWIW, I am not convinced that the current way I did it in my in-development R package is the best strategy.)   ","date":1581120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581292800,"objectID":"cad05008911eb1906a2df047646553f2","permalink":"https://www.jonesingfordata.com/post/2020-02-08-comps-study-guide/","publishdate":"2020-02-08T00:00:00Z","relpermalink":"/post/2020-02-08-comps-study-guide/","section":"post","summary":"This post is going to make a boring read.\nAs I explained in my last post, my program’s comprehensive exams are bespoke for the student. I’m going to use this post as a high-level study guide to keep me on track. I’ll update it as I get more clarification etc.\nThe basic structure of the exam is as follows:\nThere is a one-day (up to six-hour) written exam taken in-person at the university and proctored by one committee member.","tags":[],"title":"Comprehensive exam study guide","type":"post"},{"authors":null,"categories":null,"content":"My name is Tommy. I\u0026rsquo;m a member of the technical staff at In-Q-Tel and a coordinator for Data Science DC. I have an MS in mathematics and statistics from Georgetown University and a BA in economics from the College of William and Mary. I am also a PhD student in the George Mason University Department of Computational and Data Sciences. I am the author of the textmineR package for the R language. I am also a Marine Corps veteran. Terrible opinions are my own.\nCheck out my GitHub.\nYou can follow me on twitter \\@thos_jones\n","date":1573344000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573344000,"objectID":"6083a88ee3411b0d17ce02d738f69d47","permalink":"https://www.jonesingfordata.com/about/","publishdate":"2019-11-10T00:00:00Z","relpermalink":"/about/","section":"","summary":"My name is Tommy. I\u0026rsquo;m a member of the technical staff at In-Q-Tel and a coordinator for Data Science DC. I have an MS in mathematics and statistics from Georgetown University and a BA in economics from the College of William and Mary. I am also a PhD student in the George Mason University Department of Computational and Data Sciences. I am the author of the textmineR package for the R language.","tags":null,"title":"About","type":"page"},{"authors":["Tommy Jones"],"categories":null,"content":"","date":1573293600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573293600,"objectID":"d8c41f02aab52be14867b21511dd2cc3","permalink":"https://www.jonesingfordata.com/talk/2019_11_09_dcr/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2019_11_09_dcr/","section":"talk","summary":"Using a Bayesian optimizer to build better topic models.","tags":[],"title":"Optimizing Topic Models for Classification Tasks","type":"talk"},{"authors":["Tommy Jones"],"categories":null,"content":"","date":1555353000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555353000,"objectID":"b095c94f8ce21a57648350ad53f13bbb","permalink":"https://www.jonesingfordata.com/talk/2019_04_15_dsdc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2019_04_15_dsdc/","section":"talk","summary":"Actually just a 10-minute introduction to graph analytics. Very little math was involved.","tags":[],"title":"A Brief Introduction to Graph Theory","type":"talk"},{"authors":["Tommy Jones"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://www.jonesingfordata.com/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://www.jonesingfordata.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Tommy Jones"],"categories":null,"content":"","date":1541757600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541757600,"objectID":"e481fd9359112d7713098aef04b017c8","permalink":"https://www.jonesingfordata.com/talk/2018_11_09_dcr/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2018_11_09_dcr/","section":"talk","summary":"An introduction to textmineR v3.x which introduced new features.","tags":[],"title":"Mining Texts with textmineR","type":"talk"},{"authors":["Tommy Jones"],"categories":null,"content":"","date":1460917800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1460917800,"objectID":"06ef18b2e9895287729bb2854fda65a4","permalink":"https://www.jonesingfordata.com/talk/2016_04_27_spdc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2016_04_27_spdc/","section":"talk","summary":"The first time I spoke about textminR in public","tags":[],"title":"textmineR - NLP with R","type":"talk"},{"authors":["Tommy Jones","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://www.jonesingfordata.com/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Tommy Jones"],"categories":null,"content":"","date":1415817000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1415817000,"objectID":"1cfa0dfe82340df8788696f3ecfecf44","permalink":"https://www.jonesingfordata.com/talk/2014_11_12_dcnlp/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2014_11_12_dcnlp/","section":"talk","summary":"Topic models are a family of models to estimate the distribution of abstract concepts (topics) that make up a collection of documents. Over the last several years, the popularity of topic modeling has swelled. One model, Latent Dirichlet Allocation (LDA), is especially popular.\nTommy Jones will describe a range of topic modeling algorithms and how they fit into the topic modeling taxonomy. He will then focus on LDA, explaining how to tune its parameters and giving tips for building better LDA models.\nFinally, Tommy will present several open statistical questions in topic modeling, particularly LDA. Examples include LDA's inconsistency, how sample selection affects estimates, and how to best present results. Researchers have begun to tackle some of these issues, but others remain. Still, LDA and other topic models are becoming invaluable resources for researchers in many disciplines.","tags":[],"title":"Introduction to Topic Modeling with LDA and more","type":"talk"},{"authors":["Tommy Jones","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://www.jonesingfordata.com/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1489cf6c947d4e4529f35efec07a5269","permalink":"https://www.jonesingfordata.com/project/marginal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/marginal/","section":"project","summary":"R package for calculating marginal effects for arbitrary prediction models.","tags":["R","machine learning","interpretable ai"],"title":"marginal","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0861566cc892f462b0570cf2701c5e26","permalink":"https://www.jonesingfordata.com/project/mvrsquared/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/mvrsquared/","section":"project","summary":"R package implementing multivariate R-squared for topic models and other multivariate outcome models","tags":["R","machine learning"],"title":"mvrsquared","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"304e9205eec466f216c1bdc441289031","permalink":"https://www.jonesingfordata.com/project/rsquared/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/rsquared/","section":"project","summary":"Working paper of a Coefficient of Determination for Topic Models.","tags":["R-squared","topic model","latent dirichlet allocation"],"title":"R-squared for Topic Models","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"460bb8cda9ab2fe59ce6b277e0b635c4","permalink":"https://www.jonesingfordata.com/project/textminer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/textminer/","section":"project","summary":"Text mining and topic modeling in R.","tags":["R","topic model","latent dirichlet allocation"],"title":"textmineR","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7d13f67657bbdc917576a5ed26ba3d8c","permalink":"https://www.jonesingfordata.com/project/tidylda/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/tidylda/","section":"project","summary":"R package for Latent Dirichlet Allocation using ‘tidyverse’ conventions plus some of my own special stuff","tags":["R","machine learning"],"title":"tidylda","type":"project"}]